{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Lanuguage Processing\n",
    "## Fake news identification\n",
    "\n",
    "by Daniel Russotto and Christine Utendorf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment our goal is to determine if a provided article including a title and text provides real or fake news. Fake news consists of disinformation and it imposes a great threat to our today's society. Not knowing what to believe when it comes to news and information or even worse not recognizing that the information provided is not reflecting the truth can truely harm a reader. Especially through the rise of the internet and with it the rise of social media, news can be accessed at any time, any place and from many different sources. However, this also gives fake news the possibility to spread faster, wider and more successfully.\n",
    "\n",
    "Social media giant Facebook has set up a unit to identify such fake news on its platform. After being critized more than ones for doing to little against the spread of false infromation, Facebook is now \"working to stop misinformation and false news\". The company is not only working together with third-party fact-check organizations but is also applying machine learning techniques to identify such post that contain fake news (see more under the [link](https://www.facebook.com/facebookmedia/blog/working-to-stop-misinformation-and-false-news)). It is highly likely that Facebook uses Natural Language Processing and classification algorithms in order to determine if they have fraud in front of them or not.\n",
    "\n",
    "In this assignment we (Dan and Christine) are going to work on such a problem that Facebook (as well as Twitter, Youtube, and many other platforms) is facing everyday: Identifying is an article provides real or fake news. We are not going to use fact-checking in order to prove if an information is acurate, but train a machine learning algorithm to classify articles as fake or real. In order to do so we are using several concepts of natural language processing such as tokenization and lemmatization from the NLTK python library as well as machine learning concepts including logistic regression and naive bayes from the sklearn python library. For this puprpose we were provided with a training data set that includes articels that are already labled as real or fake and a test set without such labels. The goal is to train a model that is able to find a general pattern to identify fake news among articles it has never seen before (here our \"blind\" test data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import sklearn\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from functools import reduce\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/fake_or_real_news_training.csv\", quotechar='\"', header=0, sep=\",\",\n",
    "                    index_col=\"ID\", encoding='utf-8')\n",
    "test_data = pd.read_csv(\"data/fake_or_real_news_test.csv\", quotechar='\"', header=0, sep=\",\",\n",
    "                   index_col=\"ID\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set\n",
    "The training data set has 3,999 rows each representing an article. All articles are identified with a unique id, have a title, text and a label if they are fake or real. Furthermore there are the columns X1 and X2. These two columns should actually be all filled with NaN (=empty). However, 33 rows show values in X1 and 2 of these 33 also in X2. This shows that the text was not properly splitted in these cases. The separator used to splitt the csv into a dataframe is \",\" but as it seems in some of the cases this did not split all rows correctly. In the data cleaning part we will take a closer look at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3999 entries, 8476 to 9673\n",
      "Data columns (total 5 columns):\n",
      "title    3999 non-null object\n",
      "text     3999 non-null object\n",
      "label    3999 non-null object\n",
      "X1       33 non-null object\n",
      "X2       2 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 187.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label   X1   X2  \n",
       "ID                                                                        \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "875    It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data\n",
    "The test data set consists of 2321 unlabeled rows and due to the fact that the data frame only includes the unique id, the title and the actual text it seems as the text split worked well here (no X1 or X2). The train data has less than double the amount of articles in it compared to the test data set. This makes it crucial to retrain at the end the machine learning with the complete train data set due to limited data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2321 entries, 10498 to 4330\n",
      "Data columns (total 2 columns):\n",
      "title    2321 non-null object\n",
      "text     2321 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 54.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                    text  \n",
       "ID                                                        \n",
       "10498  September New Homes Sales Rise Back To 1992 Le...  \n",
       "2439   But when Congress debated and passed the Patie...  \n",
       "864    The Bernie Sanders and Ted Cruz campaigns vowe...  \n",
       "4128   Police searching for the second of two escaped...  \n",
       "662    No matter who wins California's 475 delegates ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data cleaning\n",
    "As seen in the train_data.info() we have several rows that were not correctly put in the dataframe. We are now going to fix these rows. The csv is splitted using commas. Thus a title or text that has commas in it is splitted incorrectly. A first step is to filter out the rows that are displaced and take a closer look at them. Overall, we have 33 rows with displaced values since all the rows that have values in X2 have values in X1. It is important to fix these values due to the limited number of training rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Election Day: No Legal Pot In Ohio</td>\n",
       "      <td>Democrats Lose In The South</td>\n",
       "      <td>Election Day: No Legal Pot In Ohio; Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194</th>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>Leonardo DiCaprio to the rescue?</td>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Black Hawk crashes off Florida</td>\n",
       "      <td>human remains found</td>\n",
       "      <td>(CNN) Thick fog forced authorities to suspend ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital</td>\n",
       "      <td>U.S. investigating</td>\n",
       "      <td>(CNN) Aerial bombardments blew apart a Doctors...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>US issues travel warning</td>\n",
       "      <td>A member of Al Qaeda's branch in Yemen said Fr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>Shallow 5.4 magnitude earthquake rattles centr...</td>\n",
       "      <td>shakes buildings in Rome</td>\n",
       "      <td>00 UTC © USGS Map of the earthquake's epicent...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>ICE Agent Commits Suicide in NYC</td>\n",
       "      <td>Leaves Note Revealing Gov’t Plans to Round-up...</td>\n",
       "      <td>Email Print After writing a lengthy suicide no...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9203</th>\n",
       "      <td>Political Correctness for Yuengling Brewery</td>\n",
       "      <td>What About Our Opioid Epidemic?</td>\n",
       "      <td>We Are Change \\r\\n\\r\\nIn today’s political cli...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>Poll gives Biden edge over Clinton against GOP...</td>\n",
       "      <td>VP meets with Trumka</td>\n",
       "      <td>A new national poll shows Vice President Biden...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>Russia begins airstrikes in Syria</td>\n",
       "      <td>U.S. warns of new concerns in conflict</td>\n",
       "      <td>Russian warplanes began airstrikes in Syria on...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>Trump &amp;amp</td>\n",
       "      <td>Clinton Were Very Convincing...on How Lousy t...</td>\n",
       "      <td>Let's pretend for a moment that the biggest he...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>Belgian police mount raids</td>\n",
       "      <td>prosecutors acknowledge missed opportunities</td>\n",
       "      <td>Belgian authorities missed a chance to press a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...</td>\n",
       "      <td>GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS</td>\n",
       "      <td>Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues</td>\n",
       "      <td>Brothers Were On No-Fly List</td>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues;...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>The Amish In America Commit Their Vote To Dona...</td>\n",
       "      <td>Mathematically Guaranteeing Him A Presidentia...</td>\n",
       "      <td>18 SHARE The Amish in America have committed t...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6404</th>\n",
       "      <td>#BREAKING: SECOND Assassination Attempt On Tru...</td>\n",
       "      <td>Suspect Detained (LIVE BLOG)</td>\n",
       "      <td>We Are Change \\r\\nDonald Trump on Saturday was...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>30th Infantry Division: “Work Horse of the Wes...</td>\n",
       "      <td>The Big Picture TV-211</td>\n",
       "      <td>Published on Oct 27, 2016 by Jeff Quitney The ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Planned Parenthood’s lobbying effort</td>\n",
       "      <td>pay raises for federal workers</td>\n",
       "      <td>and the future Fed rates</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10492</th>\n",
       "      <td>TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...</td>\n",
       "      <td>“THE END OF LIFE AS WE KNOW IT”</td>\n",
       "      <td>Paul Joseph Watson Senior British army officer...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>Inside The Mind Of An FBI Informant</td>\n",
       "      <td>Terri Linnell Admits Role As Gov’t Snitch</td>\n",
       "      <td>Inside The Mind Of An FBI Informant; Terri Lin...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>Gary Johnson Avoids Typical Third-Party Fade</td>\n",
       "      <td>Best Polling Since Perot in ‘92</td>\n",
       "      <td>A couple of weeks ago in this space I pushed b...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Nearly 300K New Jobs In February</td>\n",
       "      <td>Unemployment Dips To 5.5 Percent</td>\n",
       "      <td>Nearly 300K New Jobs In February; Unemployment...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>Why Trump Won</td>\n",
       "      <td>Why Clinton Lost</td>\n",
       "      <td>WashingtonsBlog \\r\\nBy Robert Parry, the inv...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>Jesse Matthew charged in Hannah Graham's murder</td>\n",
       "      <td>DA will not pursue death penalty</td>\n",
       "      <td>Jesse Matthew Jr., a former hospital worker, w...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8748</th>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRiot</td>\n",
       "      <td>Media Ignores (Video)</td>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRio...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>Jim Rogers: It’s Time To Prepare</td>\n",
       "      <td>Economic And Financial Collapse Imminent (VIDEO)</td>\n",
       "      <td>By: The Voice of Reason | Regardless of how mu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Islamic State admits defeat in Kobani</td>\n",
       "      <td>blames airstrikes</td>\n",
       "      <td>Islamic State militants have acknowledged for ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>Clinton Cries Racism Tagging Trump with KKK</td>\n",
       "      <td>Trump Says 'She Lies'</td>\n",
       "      <td>With only about 70 days left until the electio...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Suspects In Paris Magazine Attack Killed</td>\n",
       "      <td>Market Gunman And 4 Hostages Also Dead</td>\n",
       "      <td>Suspects In Paris Magazine Attack Killed; Mark...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>Chart Of The Day: Since 2009 Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>Ted Cruz launches bid</td>\n",
       "      <td>Some pundits paint him as scary extremist</td>\n",
       "      <td>Before he got to repealing ObamaCare, before h...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>State Dept. IDs 2 Americans killed in Nepal quake</td>\n",
       "      <td>2 others reportedly dead</td>\n",
       "      <td>The State Department identified two Americans ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>bursting of firecrackers suspected</td>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "599                   Election Day: No Legal Pot In Ohio   \n",
       "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "356                       Black Hawk crashes off Florida   \n",
       "2786      Afghanistan: 19 die in air attacks on hospital   \n",
       "3622   Al Qaeda rep says group directed Paris magazin...   \n",
       "7375   Shallow 5.4 magnitude earthquake rattles centr...   \n",
       "9097                    ICE Agent Commits Suicide in NYC   \n",
       "9203         Political Correctness for Yuengling Brewery   \n",
       "1602   Poll gives Biden edge over Clinton against GOP...   \n",
       "4562                   Russia begins airstrikes in Syria   \n",
       "4748                                          Trump &amp   \n",
       "3508                          Belgian police mount raids   \n",
       "7559   STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...   \n",
       "3634       The Latest On Paris Attack: Manhunt Continues   \n",
       "8470   The Amish In America Commit Their Vote To Dona...   \n",
       "6404   #BREAKING: SECOND Assassination Attempt On Tru...   \n",
       "10499  30th Infantry Division: “Work Horse of the Wes...   \n",
       "9                   Planned Parenthood’s lobbying effort   \n",
       "10492  TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...   \n",
       "10138                Inside The Mind Of An FBI Informant   \n",
       "4953        Gary Johnson Avoids Typical Third-Party Fade   \n",
       "496                     Nearly 300K New Jobs In February   \n",
       "5741                                       Why Trump Won   \n",
       "4131     Jesse Matthew charged in Hannah Graham's murder   \n",
       "8748       WATCH: Mass Shooting Occurs During #TrumpRiot   \n",
       "6717                    Jim Rogers: It’s Time To Prepare   \n",
       "2943               Islamic State admits defeat in Kobani   \n",
       "5248         Clinton Cries Racism Tagging Trump with KKK   \n",
       "3624            Suspects In Paris Magazine Attack Killed   \n",
       "6268   Chart Of The Day: Since 2009—–Recovery For The 5%   \n",
       "2738                               Ted Cruz launches bid   \n",
       "4025   State Dept. IDs 2 Americans killed in Nepal quake   \n",
       "9954   Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "599                          Democrats Lose In The South   \n",
       "10194                   Leonardo DiCaprio to the rescue?   \n",
       "356                                  human remains found   \n",
       "2786                                  U.S. investigating   \n",
       "3622                            US issues travel warning   \n",
       "7375                            shakes buildings in Rome   \n",
       "9097    Leaves Note Revealing Gov’t Plans to Round-up...   \n",
       "9203                     What About Our Opioid Epidemic?   \n",
       "1602                                VP meets with Trumka   \n",
       "4562              U.S. warns of new concerns in conflict   \n",
       "4748    Clinton Were Very Convincing...on How Lousy t...   \n",
       "3508        prosecutors acknowledge missed opportunities   \n",
       "7559        GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS   \n",
       "3634                        Brothers Were On No-Fly List   \n",
       "8470    Mathematically Guaranteeing Him A Presidentia...   \n",
       "6404                        Suspect Detained (LIVE BLOG)   \n",
       "10499                             The Big Picture TV-211   \n",
       "9                         pay raises for federal workers   \n",
       "10492                    “THE END OF LIFE AS WE KNOW IT”   \n",
       "10138          Terri Linnell Admits Role As Gov’t Snitch   \n",
       "4953                     Best Polling Since Perot in ‘92   \n",
       "496                     Unemployment Dips To 5.5 Percent   \n",
       "5741                                    Why Clinton Lost   \n",
       "4131                    DA will not pursue death penalty   \n",
       "8748                               Media Ignores (Video)   \n",
       "6717    Economic And Financial Collapse Imminent (VIDEO)   \n",
       "2943                                   blames airstrikes   \n",
       "5248                               Trump Says 'She Lies'   \n",
       "3624              Market Gunman And 4 Hostages Also Dead   \n",
       "6268                              Stagnation for the 95%   \n",
       "2738           Some pundits paint him as scary extremist   \n",
       "4025                            2 others reportedly dead   \n",
       "9954                  bursting of firecrackers suspected   \n",
       "\n",
       "                                                   label  \\\n",
       "ID                                                         \n",
       "599    Election Day: No Legal Pot In Ohio; Democrats ...   \n",
       "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "356    (CNN) Thick fog forced authorities to suspend ...   \n",
       "2786   (CNN) Aerial bombardments blew apart a Doctors...   \n",
       "3622   A member of Al Qaeda's branch in Yemen said Fr...   \n",
       "7375    00 UTC © USGS Map of the earthquake's epicent...   \n",
       "9097   Email Print After writing a lengthy suicide no...   \n",
       "9203   We Are Change \\r\\n\\r\\nIn today’s political cli...   \n",
       "1602   A new national poll shows Vice President Biden...   \n",
       "4562   Russian warplanes began airstrikes in Syria on...   \n",
       "4748   Let's pretend for a moment that the biggest he...   \n",
       "3508   Belgian authorities missed a chance to press a...   \n",
       "7559   Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...   \n",
       "3634   The Latest On Paris Attack: Manhunt Continues;...   \n",
       "8470   18 SHARE The Amish in America have committed t...   \n",
       "6404   We Are Change \\r\\nDonald Trump on Saturday was...   \n",
       "10499  Published on Oct 27, 2016 by Jeff Quitney The ...   \n",
       "9                               and the future Fed rates   \n",
       "10492  Paul Joseph Watson Senior British army officer...   \n",
       "10138  Inside The Mind Of An FBI Informant; Terri Lin...   \n",
       "4953   A couple of weeks ago in this space I pushed b...   \n",
       "496    Nearly 300K New Jobs In February; Unemployment...   \n",
       "5741     WashingtonsBlog \\r\\nBy Robert Parry, the inv...   \n",
       "4131   Jesse Matthew Jr., a former hospital worker, w...   \n",
       "8748     WATCH: Mass Shooting Occurs During #TrumpRio...   \n",
       "6717   By: The Voice of Reason | Regardless of how mu...   \n",
       "2943   Islamic State militants have acknowledged for ...   \n",
       "5248   With only about 70 days left until the electio...   \n",
       "3624   Suspects In Paris Magazine Attack Killed; Mark...   \n",
       "6268    Chart Of The Day: Since 2009 Recovery For The 5%   \n",
       "2738   Before he got to repealing ObamaCare, before h...   \n",
       "4025   The State Department identified two Americans ...   \n",
       "9954   Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                      X1    X2  \n",
       "ID                                                              \n",
       "599                                                 REAL   NaN  \n",
       "10194                                               FAKE   NaN  \n",
       "356                                                 REAL   NaN  \n",
       "2786                                                REAL   NaN  \n",
       "3622                                                REAL   NaN  \n",
       "7375                                                FAKE   NaN  \n",
       "9097                                                FAKE   NaN  \n",
       "9203                                                FAKE   NaN  \n",
       "1602                                                REAL   NaN  \n",
       "4562                                                REAL   NaN  \n",
       "4748                                                REAL   NaN  \n",
       "3508                                                REAL   NaN  \n",
       "7559                                                FAKE   NaN  \n",
       "3634                                                REAL   NaN  \n",
       "8470                                                FAKE   NaN  \n",
       "6404                                                FAKE   NaN  \n",
       "10499                                               FAKE   NaN  \n",
       "9      PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  \n",
       "10492                                               FAKE   NaN  \n",
       "10138                                               FAKE   NaN  \n",
       "4953                                                REAL   NaN  \n",
       "496                                                 REAL   NaN  \n",
       "5741                                                FAKE   NaN  \n",
       "4131                                                REAL   NaN  \n",
       "8748                                                FAKE   NaN  \n",
       "6717                                                FAKE   NaN  \n",
       "2943                                                REAL   NaN  \n",
       "5248                                                REAL   NaN  \n",
       "3624                                                REAL   NaN  \n",
       "6268                            Stagnation for the 95%    FAKE  \n",
       "2738                                                REAL   NaN  \n",
       "4025                                                REAL   NaN  \n",
       "9954                                                FAKE   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displaced_rows = train_data[train_data.X1.notnull()]\n",
    "displaced_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems that the problem lies within the title. For the 31 cases that have only one misplacement (X2 = NaN), the title was splitted into two causing that the actual label is within the X1 column and the article text in the label column. For the double splitted row with index 9, it actually seems that in the title were two commas leading to a double split. However the row with index 6268 repeats the wrongly splitted phrase just again in the label and X1. There is no sign of further text and thus this row should be excluded since it does not provide an actual article text.\n",
    "However, lets start with the rows that have one wrong column break by joining the title and the text field back together into the full title and then replace the text and label column with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data.X1.notnull(), 'title'] = train_data.loc[train_data.X1.notnull(), 'title'] + train_data.loc[train_data.X1.notnull(), 'text']\n",
    "train_data.loc[train_data.X1.notnull(), 'text'] = train_data.loc[train_data.X1.notnull(), 'label']\n",
    "train_data.loc[train_data.X1.notnull(), 'label'] = train_data.loc[train_data.X1.notnull(), 'X1']\n",
    "train_data.loc[train_data.X1.notnull(), 'X1'] = train_data.loc[train_data.X1.notnull(), 'X2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fixing the line break for the first 31 rows, we are now gonna look at the last two displaced rows. While the row with index 9 seems to be easy to fix, the row with id 6268 does not seem to actually have text but only a title. Thus first we are going to fix row 9 and then see if we have more cases in the data set where there is only a title but no text in order to decide how to deal with row 6268."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Planned Parenthood’s lobbying effort pay raise...</td>\n",
       "      <td>and the future Fed rates</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The...</td>\n",
       "      <td>Chart Of The Day: Since 2009 Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "ID                                                        \n",
       "9     Planned Parenthood’s lobbying effort pay raise...   \n",
       "6268  Chart Of The Day: Since 2009—–Recovery For The...   \n",
       "\n",
       "                                                  text  \\\n",
       "ID                                                       \n",
       "9                             and the future Fed rates   \n",
       "6268  Chart Of The Day: Since 2009 Recovery For The 5%   \n",
       "\n",
       "                                                  label    X1    X2  \n",
       "ID                                                                   \n",
       "9     PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  REAL  \n",
       "6268                           Stagnation for the 95%    FAKE  FAKE  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displaced_rows = train_data[train_data.X1.notnull()]\n",
    "displaced_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix row 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[9, 'title'] = train_data.loc[9, 'title'] + \",\" + train_data.loc[9, 'text']\n",
    "train_data.loc[9, 'text'] = train_data.loc[9, 'label']\n",
    "train_data.loc[9, 'label'] = train_data.loc[9, 'X1']\n",
    "train_data.loc[9, 'X1'] = np.nan\n",
    "train_data.loc[9, 'X2'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows with no text\n",
    "Overall we see that we have 21 rows that only have white space within their text column. Furthermore, we see that all these entries are fake, which is also the case for our displaced row 6268. Thus in order to identify these cases more easily we will insert \"zzzzz\" in the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>The Arcturian Group by Marilyn Raffaele Octobe...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>Southern Poverty Law Center Targets Anti-Jihad...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>Refugee Resettlement Watch: Swept Away In Nort...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>Michael Bloomberg Names Technological Unemploy...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>Alert News : Putins Army Is Coming For World W...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>An LDS Reader Takes A Look At Trump Accuser Je...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>America’s Senator Jeff Sessions Warns of Worse...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>Paris Migrant Campers Increase after Calais Is...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>Putins Army is coming for World war 3 against ...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714</th>\n",
       "      <td>Is your promising internet career over now Vin...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>Radio Derb Transcript For October 21 Up: The M...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>A Reader Refers Us To Englishman Pat Condell O...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>“Donald Trump And The Rise Of White Identity I...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>Hope for the best, prepare for the worst…</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>The Comey Confrontation: In Our New Third-Worl...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>NATIONAL REVIEW, Conservatism Inc., Plan To Ca...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>Thomas Frank Explores Whether Hillary Clinton ...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>Democrats Playing Class Card To Split the Whit...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>Comment software has been rolled back to old v...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9070</th>\n",
       "      <td>Round Up the Unusual Suspects: Moneyball Nerds...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title text label   X1   X2\n",
       "ID                                                                           \n",
       "5530   The Arcturian Group by Marilyn Raffaele Octobe...       FAKE  NaN  NaN\n",
       "8332   MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...       FAKE  NaN  NaN\n",
       "9314   Southern Poverty Law Center Targets Anti-Jihad...       FAKE  NaN  NaN\n",
       "10304  Refugee Resettlement Watch: Swept Away In Nort...       FAKE  NaN  NaN\n",
       "9474   Michael Bloomberg Names Technological Unemploy...       FAKE  NaN  NaN\n",
       "5802   Alert News : Putins Army Is Coming For World W...       FAKE  NaN  NaN\n",
       "9564   An LDS Reader Takes A Look At Trump Accuser Je...       FAKE  NaN  NaN\n",
       "5752   America’s Senator Jeff Sessions Warns of Worse...       FAKE  NaN  NaN\n",
       "8816   Paris Migrant Campers Increase after Calais Is...       FAKE  NaN  NaN\n",
       "7525   Putins Army is coming for World war 3 against ...       FAKE  NaN  NaN\n",
       "6714   Is your promising internet career over now Vin...       FAKE  NaN  NaN\n",
       "5776   Radio Derb Transcript For October 21 Up: The M...       FAKE  NaN  NaN\n",
       "8055   A Reader Refers Us To Englishman Pat Condell O...       FAKE  NaN  NaN\n",
       "10193  “Donald Trump And The Rise Of White Identity I...       FAKE  NaN  NaN\n",
       "5715           Hope for the best, prepare for the worst…       FAKE  NaN  NaN\n",
       "6733   The Comey Confrontation: In Our New Third-Worl...       FAKE  NaN  NaN\n",
       "5367   NATIONAL REVIEW, Conservatism Inc., Plan To Ca...       FAKE  NaN  NaN\n",
       "5427   Thomas Frank Explores Whether Hillary Clinton ...       FAKE  NaN  NaN\n",
       "8240   Democrats Playing Class Card To Split the Whit...       FAKE  NaN  NaN\n",
       "7048   Comment software has been rolled back to old v...       FAKE  NaN  NaN\n",
       "9070   Round Up the Unusual Suspects: Moneyball Nerds...       FAKE  NaN  NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = train_data[train_data.text == ' ']\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace empty text with zzzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>The Arcturian Group by Marilyn Raffaele Octobe...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>Southern Poverty Law Center Targets Anti-Jihad...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>Refugee Resettlement Watch: Swept Away In Nort...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>Michael Bloomberg Names Technological Unemploy...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>Alert News : Putins Army Is Coming For World W...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>An LDS Reader Takes A Look At Trump Accuser Je...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>America’s Senator Jeff Sessions Warns of Worse...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>Paris Migrant Campers Increase after Calais Is...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>Putins Army is coming for World war 3 against ...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714</th>\n",
       "      <td>Is your promising internet career over now Vin...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>Radio Derb Transcript For October 21 Up: The M...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>A Reader Refers Us To Englishman Pat Condell O...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>“Donald Trump And The Rise Of White Identity I...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>Hope for the best, prepare for the worst…</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>The Comey Confrontation: In Our New Third-Worl...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>NATIONAL REVIEW, Conservatism Inc., Plan To Ca...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>Thomas Frank Explores Whether Hillary Clinton ...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>Democrats Playing Class Card To Split the Whit...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>Comment software has been rolled back to old v...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9070</th>\n",
       "      <td>Round Up the Unusual Suspects: Moneyball Nerds...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title   text label    X1  \\\n",
       "ID                                                                            \n",
       "5530   The Arcturian Group by Marilyn Raffaele Octobe...  zzzzz  FAKE   NaN   \n",
       "8332   MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...  zzzzz  FAKE   NaN   \n",
       "9314   Southern Poverty Law Center Targets Anti-Jihad...  zzzzz  FAKE   NaN   \n",
       "10304  Refugee Resettlement Watch: Swept Away In Nort...  zzzzz  FAKE   NaN   \n",
       "9474   Michael Bloomberg Names Technological Unemploy...  zzzzz  FAKE   NaN   \n",
       "5802   Alert News : Putins Army Is Coming For World W...  zzzzz  FAKE   NaN   \n",
       "9564   An LDS Reader Takes A Look At Trump Accuser Je...  zzzzz  FAKE   NaN   \n",
       "5752   America’s Senator Jeff Sessions Warns of Worse...  zzzzz  FAKE   NaN   \n",
       "8816   Paris Migrant Campers Increase after Calais Is...  zzzzz  FAKE   NaN   \n",
       "7525   Putins Army is coming for World war 3 against ...  zzzzz  FAKE   NaN   \n",
       "6714   Is your promising internet career over now Vin...  zzzzz  FAKE   NaN   \n",
       "5776   Radio Derb Transcript For October 21 Up: The M...  zzzzz  FAKE   NaN   \n",
       "8055   A Reader Refers Us To Englishman Pat Condell O...  zzzzz  FAKE   NaN   \n",
       "10193  “Donald Trump And The Rise Of White Identity I...  zzzzz  FAKE   NaN   \n",
       "5715           Hope for the best, prepare for the worst…  zzzzz  FAKE   NaN   \n",
       "6733   The Comey Confrontation: In Our New Third-Worl...  zzzzz  FAKE   NaN   \n",
       "5367   NATIONAL REVIEW, Conservatism Inc., Plan To Ca...  zzzzz  FAKE   NaN   \n",
       "5427   Thomas Frank Explores Whether Hillary Clinton ...  zzzzz  FAKE   NaN   \n",
       "8240   Democrats Playing Class Card To Split the Whit...  zzzzz  FAKE   NaN   \n",
       "6268   Chart Of The Day: Since 2009—–Recovery For The...  zzzzz  FAKE  FAKE   \n",
       "7048   Comment software has been rolled back to old v...  zzzzz  FAKE   NaN   \n",
       "9070   Round Up the Unusual Suspects: Moneyball Nerds...  zzzzz  FAKE   NaN   \n",
       "\n",
       "         X2  \n",
       "ID           \n",
       "5530    NaN  \n",
       "8332    NaN  \n",
       "9314    NaN  \n",
       "10304   NaN  \n",
       "9474    NaN  \n",
       "5802    NaN  \n",
       "9564    NaN  \n",
       "5752    NaN  \n",
       "8816    NaN  \n",
       "7525    NaN  \n",
       "6714    NaN  \n",
       "5776    NaN  \n",
       "8055    NaN  \n",
       "10193   NaN  \n",
       "5715    NaN  \n",
       "6733    NaN  \n",
       "5367    NaN  \n",
       "5427    NaN  \n",
       "8240    NaN  \n",
       "6268   FAKE  \n",
       "7048    NaN  \n",
       "9070    NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace text for specific row\n",
    "train_data.loc[6268, 'text'] = \"zzzzz\"\n",
    "train_data.loc[6268, 'label'] = train_data.loc[6268, 'X1']\n",
    "\n",
    "\n",
    "# Fill empty text columns\n",
    "train_data[\"text\"] = train_data.apply(lambda row: row[\"text\"].strip(), axis=1).replace(\"\", \"zzzzz\")\n",
    "\n",
    "# Show zzzzz rows\n",
    "train_data[train_data[\"text\"] == \"zzzzz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3999 entries, 8476 to 9673\n",
      "Data columns (total 5 columns):\n",
      "title    3999 non-null object\n",
      "text     3999 non-null object\n",
      "label    3999 non-null object\n",
      "X1       1 non-null object\n",
      "X2       1 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 347.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fixing the displaced rows we can now delete the columns X1 and X2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \n",
       "ID                                                              \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875    It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[['title', 'text','label']]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set\n",
    "Eventhough the test data set does not show a splitting problem there might be rows that do not have anything in their text column. Thus we are going to check this and then fill the empty columns like we did for the train data set. Apparently there are 15 rows in which the text column is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10414</th>\n",
       "      <td>More on Trump’s Populism and How It Can Be Con...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>Radio Derb transcript for October 29th is up: ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8601</th>\n",
       "      <td>Pro-sovereignty Legislators Demand That Admini...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>World War 3?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>A Mormon Reader Says Most Mormons Will Still B...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254</th>\n",
       "      <td>Paris: Riot Police Flatten Invader Camp</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>Huma Abedin’s Muslim Dad</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9526</th>\n",
       "      <td>Hillary is Sick &amp; Tired of Suffering from Wein...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6800</th>\n",
       "      <td>Automation: Robots from Korea to America Are R...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>WORLD WAR 3 – HILLARY V.S. TRUMP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>A Fifth Clinton Presidency? Hill, No!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>Huma’s Weiner Dogs Hillary</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6127</th>\n",
       "      <td>Radio Derb: Peak White Guilt, PC Now To The LE...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7043</th>\n",
       "      <td>Hillary’s High Crimes &amp; Misdemeanors Threaten ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9337</th>\n",
       "      <td>Radio Derb Is On The Air–Leonardo And Brazil’s...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title text\n",
       "ID                                                           \n",
       "10414  More on Trump’s Populism and How It Can Be Con...     \n",
       "6311   Radio Derb transcript for October 29th is up: ...     \n",
       "8601   Pro-sovereignty Legislators Demand That Admini...     \n",
       "8626                                        World War 3?     \n",
       "8548   A Mormon Reader Says Most Mormons Will Still B...     \n",
       "6254             Paris: Riot Police Flatten Invader Camp     \n",
       "8875                            Huma Abedin’s Muslim Dad     \n",
       "9526   Hillary is Sick & Tired of Suffering from Wein...     \n",
       "6800   Automation: Robots from Korea to America Are R...     \n",
       "6773                    WORLD WAR 3 – HILLARY V.S. TRUMP     \n",
       "9467               A Fifth Clinton Presidency? Hill, No!     \n",
       "5324                          Huma’s Weiner Dogs Hillary     \n",
       "6127   Radio Derb: Peak White Guilt, PC Now To The LE...     \n",
       "7043   Hillary’s High Crimes & Misdemeanors Threaten ...     \n",
       "9337   Radio Derb Is On The Air–Leonardo And Brazil’s...     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text_test = test_data[test_data.text == ' ']\n",
    "empty_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"text\"] = test_data.apply(lambda row: row[\"text\"].strip(), axis=1).replace(\"\", \"zzzzz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on a classification problem, it is important to look at the target distribution. Highly imbalanced targets need resampling methods in order to train a well-working machine learning model. Thus our first step in terms of data exploration is to check the amount of fake and real labels within our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    2008\n",
       "text     2008\n",
       "label    2008\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.label == \"REAL\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    1991\n",
       "text     1991\n",
       "label    1991\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.label == \"FAKE\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is almost equally distributed with 2008 real and 1990 fake articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preparation\n",
    "\n",
    "In order to work with the title and text, the columns need to be cleaned. We follow several steps in order to prepare our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 POS tagging\n",
    "The first step before we go into normalizing our texts, is part-of-speech (POS) tagging. POS tagging determines which role a word plays within a sentence. The four most common tags are noun, verb, adjective and adverb. \n",
    "\n",
    "We are using POS tagging in order to tag all the words within our title and text to use this as input for the following lemmatization. An easy approach is to apply lemmatization tagging all words either as a verb or a noun. However, this will not always result in a very desirable lemmatization output. Thus we created two function that use the nltk pos_tag function to pos tag the words (postag_text) and then based on these tags we assigned these tags to the four main tag categories (get_wordnet_pos) which are then used beside the word itself as input for our lemmatization process. \n",
    "\n",
    "Within the postag_text function it can be determined to remove stopwords, lowercase the words and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #by default is noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postag_text(txt, rm_stopwords=True, lowertext=True, rm_punct=True):\n",
    "    if rm_punct:\n",
    "        txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "        \n",
    "    tokenized = word_tokenize(txt)\n",
    "    if lowertext:\n",
    "        tokenized = list(map(lambda word: word.lower(), tokenized))\n",
    "        \n",
    "    if rm_stopwords:\n",
    "        sw = set(stopwords.words('english'))\n",
    "        tokenized = list(filter(lambda word: word not in sw, tokenized))\n",
    "        \n",
    "    tags = nltk.pos_tag(tokenized)\n",
    "    return list(map(lambda tag_info: (tag_info[0], get_wordnet_pos(tag_info[1])), tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lemmatizing\n",
    "\n",
    "Lemmatizing is trying to find the root of a word and replaces this word with its root. For verbs this is the infinitive thus are/is/was etc. is turned to \"be\". Four nouns the root is usually the singular form. However, there are also other noun transformation such as working, which has the root work. Lemmatizing transforms mainly nouns and in particular verbs, however also adjective can be affected for example by using comparatives and superlatives.\n",
    "\n",
    "We used the nltk WordNetLemmatizer for lemmatizing our texts. The function we created first tags all the words, which are already lowercased. We decided not to exclude stopwords in this particular step since we are going to use the clean text also to create n-grams in which we think stopwords can actually be helpful. For the same reason punctuation is not removed at this point since for n-grams this is needed due to the fact that n-grams should not go over the end of a sentence. However when we use usual unigrams for vectorization stopwords and punctuation (actually everthing that is not a letter) will be removed. Luckily the WordNetLemmatizer is actually able to lemmatize a word at the end of a sentence eventhough it is connected to a period, question mark etc. (see below).\n",
    "\n",
    "The next step in this function is to actually lemmatize the words and here as said before we did not include a default tag but used the actually assigned tag from our POS tagging process. At the end the function returns the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, lowertext=True, rm_punct=False):\n",
    "    wnlt = WordNetLemmatizer()\n",
    "    \n",
    "    tagged = postag_text(text, rm_stopwords=False, rm_punct=rm_punct)\n",
    "    \n",
    "    lemmatized_list = list(map(lambda tag: wnlt.lemmatize(tag[0], pos=tag[1]), tagged))\n",
    "    \n",
    "    lemmatized_text = reduce(lambda x, y: x + \" \" + y, lemmatized_list)\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog run , he run , she be drink , people be sit\n",
      "dog run , he run , she be drink , people be sit ?\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting\"))\n",
    "print(lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming\n",
    "The next step after lemmatizing is stemming. Stemming is a much more \"brutal\" transformation than lemmatizing since stemming actually cuts off words. Most of the time from what we see stemming is applied after lemmatization if the POS tag 'verb' was used for all words in the text. For example, if lemmatization is applied and the default is verb all plural nouns are not transformed to their singular root. However, if stemming is afterwards applied the stemmer will take care of this problem. Since we use individual POS tags for each word we are not facing this problem. However, in order to reduce the overall lexical diversity of our text and to be able to identify words with the actual same meaning we also apply stemming in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, lowertext=True, rm_punct=False):\n",
    "    pst = PorterStemmer()\n",
    "    \n",
    "    if rm_punct:\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    \n",
    "    if lowertext:\n",
    "        tokenized = list(map(lambda word: word.lower(), tokenized))\n",
    "    \n",
    "    stemmed = reduce(lambda x, y: x + \" \" + y, list(map(lambda word: pst.stem(word), tokenized)))\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create cleaned columns\n",
    "Now we are going to apply first lemmatizing (incl. POS tagging) and then stemming to our title and text columns. In order to do so we created a new function that combines those two steps. As seen in the example, while stemming only cuts of the word endings, including lemmatizing actually helps to find the root of words such as be or child.\n",
    "\n",
    "For our dataframe we are now creating not only the cleaned title and text column by using lemmatizing and stemming but also combine the two cleaned columns into a third column that merges title and text.\n",
    "\n",
    "Within the clean columns we still have stop words and punctuation but all the words are already lowercased. Title and text are also separated by a period. As said before we are doing this for the later creation of n-grams and due to the fact that the vectorizers actually are able to remove stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lemmatize_text(text, lowertext=True, rm_punct=False):\n",
    "    lemmatized = lemmatize_text(text, lowertext=lowertext, rm_punct=rm_punct)\n",
    "    return stem_text(lemmatized, lowertext=lowertext, rm_punct=rm_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col(df, col):\n",
    "    return df.apply(lambda row: stem_lemmatize_text(row[col]), axis=1).to_frame(name=\"clean_{}\".format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog run , he ran , she is drink , peopl are sit , children were game\n",
      "dog run , he run , she be drink , people be sit , child be game\n",
      "dog run , he run , she be drink , peopl be sit , child be game\n"
     ]
    }
   ],
   "source": [
    "print(stem_text(\"Dogs run, he ran, she is drinking, people are sitting, children were gaming\"))\n",
    "print(lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting, children were gaming\"))\n",
    "print(stem_lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting, children were gaming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillari ’ s fear</td>\n",
       "      <td>daniel greenfield , a shillman journal fellow ...</td>\n",
       "      <td>you can smell hillari ’ s fear. daniel greenfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi</td>\n",
       "      <td>u.s. secretari of state john f. kerri say mond...</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi. u.s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>— kayde king ( @ kaydeek ) novemb 9 , 2016 the...</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battl of new york : whi thi primari matter</td>\n",
       "      <td>it 's primari day in new york and front-runn h...</td>\n",
       "      <td>the battl of new york : whi thi primari matter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0                     you can smell hillari ’ s fear   \n",
       "1  watch the exact moment paul ryan commit polit ...   \n",
       "2          kerri to go to pari in gestur of sympathi   \n",
       "3  berni support on twitter erupt in anger agains...   \n",
       "4     the battl of new york : whi thi primari matter   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  daniel greenfield , a shillman journal fellow ...   \n",
       "1  googl pinterest digg linkedin reddit stumbleup...   \n",
       "2  u.s. secretari of state john f. kerri say mond...   \n",
       "3  — kayde king ( @ kaydeek ) novemb 9 , 2016 the...   \n",
       "4  it 's primari day in new york and front-runn h...   \n",
       "\n",
       "                                      clean_combined  \n",
       "0  you can smell hillari ’ s fear. daniel greenfi...  \n",
       "1  watch the exact moment paul ryan commit polit ...  \n",
       "2  kerri to go to pari in gestur of sympathi. u.s...  \n",
       "3  berni support on twitter erupt in anger agains...  \n",
       "4  the battl of new york : whi thi primari matter...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    train_data_clean = pd.read_csv(\"data/train_data_clean.csv\")\n",
    "except:\n",
    "    print(\"File not found. Generating clean text...\")\n",
    "    clean_text = clean_col(train_data, \"text\")\n",
    "    clean_title = clean_col(train_data, \"title\")\n",
    "    clean_combined = clean_title[\"clean_title\"] + \". \" + clean_text[\"clean_text\"]\n",
    "    clean_combined = clean_combined.rename(\"clean_combined\")\n",
    "\n",
    "    train_data_clean = pd.concat([train_data,\n",
    "                            clean_title,\n",
    "                            clean_text,\n",
    "                            clean_combined\n",
    "                           ], axis=1)\n",
    "\n",
    "    train_data_clean.to_csv(\"data/train_data_clean.csv\")\n",
    "\n",
    "train_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 level !</td>\n",
       "      <td>septemb new home sale rise back to 1992 level ...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 leve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>but when congress debat and pass the patient p...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>the berni sander and ted cruz campaign vow to ...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>polic search for the second of two escap priso...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>no matter who win california 's 475 deleg on t...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...   \n",
       "1  But when Congress debated and passed the Patie...   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "3  Police searching for the second of two escaped...   \n",
       "4  No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  septemb new home sale rise——-back to 1992 level !   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...   \n",
       "2  sander , cruz resist pressur after ny loss , v...   \n",
       "3  surviv escap prison like fatigu and prone to m...   \n",
       "4  clinton and sander neck and neck in california...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  septemb new home sale rise back to 1992 level ...   \n",
       "1  but when congress debat and pass the patient p...   \n",
       "2  the berni sander and ted cruz campaign vow to ...   \n",
       "3  polic search for the second of two escap priso...   \n",
       "4  no matter who win california 's 475 deleg on t...   \n",
       "\n",
       "                                      clean_combined  \n",
       "0  septemb new home sale rise——-back to 1992 leve...  \n",
       "1  whi the obamacar doomsday cult ca n't admit it...  \n",
       "2  sander , cruz resist pressur after ny loss , v...  \n",
       "3  surviv escap prison like fatigu and prone to m...  \n",
       "4  clinton and sander neck and neck in california...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    test_data_clean = pd.read_csv(\"data/test_data_clean.csv\")\n",
    "except:\n",
    "    print(\"File not found. Generating clean text...\")\n",
    "    clean_text = clean_col(test_data, \"text\")\n",
    "    clean_title = clean_col(test_data, \"title\")\n",
    "    clean_combined = clean_title[\"clean_title\"] + \". \" + clean_text[\"clean_text\"]\n",
    "    clean_combined = clean_combined.rename(\"clean_combined\")\n",
    "\n",
    "    test_data_clean = pd.concat([test_data,\n",
    "                            clean_title,\n",
    "                            clean_text,\n",
    "                            clean_combined\n",
    "                           ], axis=1)\n",
    "\n",
    "    test_data_clean.to_csv(\"data/test_data_clean.csv\")\n",
    "\n",
    "test_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data set we are going to create several features based on our columns. We are not going to include all features right aways or all at the same time but since we are already working with the data it makes sense to start creating features that will later be included and tried within the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 POS-tag distribution\n",
    "\n",
    "The first feature that we created is related to the POS tags within the title and the text. As seen before in our data preparation part we created a function to tag the title and the text using four different tags. For each title as well as each text we are now using these tags and calculate the actual distribution of nouns, verbs, adjectives and adverbs within each title and each text. These number range from 0 to 1 reflecting the percentage of the title or text that are reflected by one of the four tags. We are not going create these features for the combined column consisting of title and text since most of the time the distribution will be close to the one from the text column. Overall we end up with eight new columns (=features), four of them for title and four of them for text. These four columns types represent the respecitve percentage of one of the four tags within the title or text.\n",
    "\n",
    "In order to create this distribution the text was not only lowercased but also only letter were included as well as stop words were removed. We took these steps in order to only include words that hold actual informative value and calculate the distribution based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_counts(pos_tags):\n",
    "    counts = {}\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in counts:\n",
    "            counts[tag] += 1\n",
    "        else:\n",
    "            counts[tag] = 1\n",
    "    return counts\n",
    "\n",
    "def tag_dists(counts):\n",
    "    total_words = sum(counts.values())\n",
    "    dists = {}\n",
    "    for tag, count in counts.items():\n",
    "        dists[tag] = count / total_words\n",
    "    return dists\n",
    "\n",
    "def create_dists_series(text):\n",
    "    return tag_dists(tag_counts(postag_text(text)))\n",
    "\n",
    "def create_dists_df(df, col):\n",
    "    return pd.DataFrame(list(df.apply(lambda row: create_dists_series(row[col]), axis=1)))\n",
    "    #     as_series = df.apply(lambda row: create_dists_row(row[\"text\"]), axis=1)\n",
    "\n",
    "    #     pd.Dataframe({\"n\": as_series[\"n\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title_dists(df):\n",
    "    df = create_dists_df(df, \"title\")\n",
    "\n",
    "    df = df.rename(columns={'a': 'title_adj_dist',\n",
    "                            'n': 'title_noun_dist',\n",
    "                            'v': 'title_verb_dist',\n",
    "                            'r': 'title_adverb_dist'})\n",
    "    # Titles are short, certain word types may not appear\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_text_dists(df):\n",
    "    df = create_dists_df(df, \"text\")\n",
    "\n",
    "    df = df.rename(columns={'a': 'text_adj_dist',\n",
    "                            'n': 'text_noun_dist',\n",
    "                            'v': 'text_verb_dist',\n",
    "                            'r': 'text_adverb_dist'})\n",
    "    # There are some articles with basically no content (e.g. just a date for a picture)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillari ’ s fear</td>\n",
       "      <td>daniel greenfield , a shillman journal fellow ...</td>\n",
       "      <td>you can smell hillari ’ s fear. daniel greenfi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210448</td>\n",
       "      <td>0.505970</td>\n",
       "      <td>0.073134</td>\n",
       "      <td>0.210448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.545833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.220833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi</td>\n",
       "      <td>u.s. secretari of state john f. kerri say mond...</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi. u.s...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.514523</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.207469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>— kayde king ( @ kaydeek ) novemb 9 , 2016 the...</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>0.497942</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.234568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battl of new york : whi thi primari matter</td>\n",
       "      <td>it 's primari day in new york and front-runn h...</td>\n",
       "      <td>the battl of new york : whi thi primari matter...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196629</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.247191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0   8476                       You Can Smell Hillary’s Fear   \n",
       "1      1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2      2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3      3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4      4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0                     you can smell hillari ’ s fear   \n",
       "1  watch the exact moment paul ryan commit polit ...   \n",
       "2          kerri to go to pari in gestur of sympathi   \n",
       "3  berni support on twitter erupt in anger agains...   \n",
       "4     the battl of new york : whi thi primari matter   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  daniel greenfield , a shillman journal fellow ...   \n",
       "1  googl pinterest digg linkedin reddit stumbleup...   \n",
       "2  u.s. secretari of state john f. kerri say mond...   \n",
       "3  — kayde king ( @ kaydeek ) novemb 9 , 2016 the...   \n",
       "4  it 's primari day in new york and front-runn h...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  you can smell hillari ’ s fear. daniel greenfi...        0.333333   \n",
       "1  watch the exact moment paul ryan commit polit ...        0.181818   \n",
       "2  kerri to go to pari in gestur of sympathi. u.s...        0.200000   \n",
       "3  berni support on twitter erupt in anger agains...        0.125000   \n",
       "4  the battl of new york : whi thi primari matter...        0.400000   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.666667                0.0         0.000000       0.210448   \n",
       "1         0.727273                0.0         0.090909       0.150000   \n",
       "2         0.600000                0.0         0.200000       0.253112   \n",
       "3         0.625000                0.0         0.250000       0.226337   \n",
       "4         0.600000                0.0         0.000000       0.196629   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  \n",
       "0        0.505970          0.073134        0.210448  \n",
       "1        0.545833          0.083333        0.220833  \n",
       "2        0.514523          0.024896        0.207469  \n",
       "3        0.497942          0.041152        0.234568  \n",
       "4        0.528090          0.028090        0.247191  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean_dist = pd.concat([train_data_clean.reset_index(),\n",
    "                                   create_title_dists(train_data_clean), \n",
    "                                   create_text_dists(train_data_clean)], \n",
    "                                  axis=1)\n",
    "train_data_clean_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 level !</td>\n",
       "      <td>septemb new home sale rise back to 1992 level ...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 leve...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.196429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>but when congress debat and pass the patient p...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189369</td>\n",
       "      <td>0.508306</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.225914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>the berni sander and ted cruz campaign vow to ...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.542923</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.245940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>polic search for the second of two escap priso...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.143087</td>\n",
       "      <td>0.540193</td>\n",
       "      <td>0.059486</td>\n",
       "      <td>0.257235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>no matter who win california 's 475 deleg on t...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.187898</td>\n",
       "      <td>0.506369</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.238854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1      1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2      2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3      3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4      4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...   \n",
       "1  But when Congress debated and passed the Patie...   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "3  Police searching for the second of two escaped...   \n",
       "4  No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  septemb new home sale rise——-back to 1992 level !   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...   \n",
       "2  sander , cruz resist pressur after ny loss , v...   \n",
       "3  surviv escap prison like fatigu and prone to m...   \n",
       "4  clinton and sander neck and neck in california...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  septemb new home sale rise back to 1992 level ...   \n",
       "1  but when congress debat and pass the patient p...   \n",
       "2  the berni sander and ted cruz campaign vow to ...   \n",
       "3  polic search for the second of two escap priso...   \n",
       "4  no matter who win california 's 475 deleg on t...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  septemb new home sale rise——-back to 1992 leve...        0.142857   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...        0.200000   \n",
       "2  sander , cruz resist pressur after ny loss , v...        0.222222   \n",
       "3  surviv escap prison like fatigu and prone to m...        0.222222   \n",
       "4  clinton and sander neck and neck in california...        0.166667   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.428571           0.142857         0.285714       0.214286   \n",
       "1         0.800000           0.000000         0.000000       0.189369   \n",
       "2         0.555556           0.000000         0.222222       0.155452   \n",
       "3         0.333333           0.000000         0.444444       0.143087   \n",
       "4         0.333333           0.166667         0.333333       0.187898   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  \n",
       "0        0.517857          0.071429        0.196429  \n",
       "1        0.508306          0.076412        0.225914  \n",
       "2        0.542923          0.055684        0.245940  \n",
       "3        0.540193          0.059486        0.257235  \n",
       "4        0.506369          0.066879        0.238854  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_clean_dist = pd.concat([test_data_clean.reset_index(),\n",
    "                                   create_title_dists(test_data_clean), \n",
    "                                   create_text_dists(test_data_clean)], \n",
    "                                  axis=1)\n",
    "test_data_clean_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lexical diversity\n",
    "\n",
    "The next feature is the lexical diversity, which reflects the ratio of different unique words/words stems to the total number of words within a text. The score is between 0 and 1 and the higher the lexcial diversity the more unique different words are included within a text. Lexical diversity belongs to the concept of lexical richness, for example while 'animal', 'beast', 'creature' can all discribe the same concept, these words are not used in the same way or concept. \n",
    "\n",
    "\n",
    "One of our thoughts is that real news should be more linked to fact based reporting while fake news rather use catchy phrases, the lexical diversity score for real news might tend to be higher compared to fake news. Even some rethorical techniques make use of repeating a word several time throughout a speech. Our suspision would be that those techniques are more used in fake than in real news articles. \n",
    "\n",
    "\n",
    "In order to create the lexical diversity score we used the clean_combined column that stores lemmatized and stemmed text and title. Furthermore, we only used letters (removing special characters and numbers) and excluded stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    # remove everything except letter (including numbers)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    sw = set(stopwords.words('english'))\n",
    "    tokenized = list(filter(lambda word: word not in sw, tokenized))\n",
    "    \n",
    "    try:\n",
    "        ld = len(set(tokenized)) / len(tokenized)\n",
    "    except ZeroDivisionError as e: # Can happen if the entire body is numbers/date\n",
    "        ld = 1.0\n",
    "    \n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div_col(df):\n",
    "    return df.apply(lambda row: lexical_diversity(row[\"clean_combined\"]), axis=1).rename(\"lexical_diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "      <th>lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillari ’ s fear</td>\n",
       "      <td>daniel greenfield , a shillman journal fellow ...</td>\n",
       "      <td>you can smell hillari ’ s fear. daniel greenfi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210448</td>\n",
       "      <td>0.505970</td>\n",
       "      <td>0.073134</td>\n",
       "      <td>0.210448</td>\n",
       "      <td>0.592754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.545833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.220833</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi</td>\n",
       "      <td>u.s. secretari of state john f. kerri say mond...</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi. u.s...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.514523</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.207469</td>\n",
       "      <td>0.678431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>— kayde king ( @ kaydeek ) novemb 9 , 2016 the...</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>0.497942</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.699620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battl of new york : whi thi primari matter</td>\n",
       "      <td>it 's primari day in new york and front-runn h...</td>\n",
       "      <td>the battl of new york : whi thi primari matter...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196629</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0   8476                       You Can Smell Hillary’s Fear   \n",
       "1      1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2      2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3      3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4      4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0                     you can smell hillari ’ s fear   \n",
       "1  watch the exact moment paul ryan commit polit ...   \n",
       "2          kerri to go to pari in gestur of sympathi   \n",
       "3  berni support on twitter erupt in anger agains...   \n",
       "4     the battl of new york : whi thi primari matter   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  daniel greenfield , a shillman journal fellow ...   \n",
       "1  googl pinterest digg linkedin reddit stumbleup...   \n",
       "2  u.s. secretari of state john f. kerri say mond...   \n",
       "3  — kayde king ( @ kaydeek ) novemb 9 , 2016 the...   \n",
       "4  it 's primari day in new york and front-runn h...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  you can smell hillari ’ s fear. daniel greenfi...        0.333333   \n",
       "1  watch the exact moment paul ryan commit polit ...        0.181818   \n",
       "2  kerri to go to pari in gestur of sympathi. u.s...        0.200000   \n",
       "3  berni support on twitter erupt in anger agains...        0.125000   \n",
       "4  the battl of new york : whi thi primari matter...        0.400000   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.666667                0.0         0.000000       0.210448   \n",
       "1         0.727273                0.0         0.090909       0.150000   \n",
       "2         0.600000                0.0         0.200000       0.253112   \n",
       "3         0.625000                0.0         0.250000       0.226337   \n",
       "4         0.600000                0.0         0.000000       0.196629   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  lexical_diversity  \n",
       "0        0.505970          0.073134        0.210448           0.592754  \n",
       "1        0.545833          0.083333        0.220833           0.763158  \n",
       "2        0.514523          0.024896        0.207469           0.678431  \n",
       "3        0.497942          0.041152        0.234568           0.699620  \n",
       "4        0.528090          0.028090        0.247191           0.625000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_features = pd.concat([train_data_clean_dist, lex_div_col(train_data_clean_dist)], axis=1)\n",
    "train_all_features.to_csv(\"data/train_all_features.csv\")\n",
    "train_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "      <th>lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 level !</td>\n",
       "      <td>septemb new home sale rise back to 1992 level ...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 leve...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.784615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>but when congress debat and pass the patient p...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189369</td>\n",
       "      <td>0.508306</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.225914</td>\n",
       "      <td>0.701258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>the berni sander and ted cruz campaign vow to ...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.542923</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.245940</td>\n",
       "      <td>0.582781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>polic search for the second of two escap priso...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.143087</td>\n",
       "      <td>0.540193</td>\n",
       "      <td>0.059486</td>\n",
       "      <td>0.257235</td>\n",
       "      <td>0.570552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>no matter who win california 's 475 deleg on t...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.187898</td>\n",
       "      <td>0.506369</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.238854</td>\n",
       "      <td>0.563636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1      1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2      2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3      3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4      4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...   \n",
       "1  But when Congress debated and passed the Patie...   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "3  Police searching for the second of two escaped...   \n",
       "4  No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  septemb new home sale rise——-back to 1992 level !   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...   \n",
       "2  sander , cruz resist pressur after ny loss , v...   \n",
       "3  surviv escap prison like fatigu and prone to m...   \n",
       "4  clinton and sander neck and neck in california...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  septemb new home sale rise back to 1992 level ...   \n",
       "1  but when congress debat and pass the patient p...   \n",
       "2  the berni sander and ted cruz campaign vow to ...   \n",
       "3  polic search for the second of two escap priso...   \n",
       "4  no matter who win california 's 475 deleg on t...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  septemb new home sale rise——-back to 1992 leve...        0.142857   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...        0.200000   \n",
       "2  sander , cruz resist pressur after ny loss , v...        0.222222   \n",
       "3  surviv escap prison like fatigu and prone to m...        0.222222   \n",
       "4  clinton and sander neck and neck in california...        0.166667   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.428571           0.142857         0.285714       0.214286   \n",
       "1         0.800000           0.000000         0.000000       0.189369   \n",
       "2         0.555556           0.000000         0.222222       0.155452   \n",
       "3         0.333333           0.000000         0.444444       0.143087   \n",
       "4         0.333333           0.166667         0.333333       0.187898   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  lexical_diversity  \n",
       "0        0.517857          0.071429        0.196429           0.784615  \n",
       "1        0.508306          0.076412        0.225914           0.701258  \n",
       "2        0.542923          0.055684        0.245940           0.582781  \n",
       "3        0.540193          0.059486        0.257235           0.570552  \n",
       "4        0.506369          0.066879        0.238854           0.563636  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_features = pd.concat([test_data_clean_dist, lex_div_col(test_data_clean_dist)], axis=1)\n",
    "test_all_features.to_csv(\"data/test_all_features.csv\")\n",
    "test_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing our train and test data sets we are going to vectorize our texts. For machine learning models in NLP we actually transform our texts into numerical feature vectors. These vectors shows the occurence of a word/bag of words. To create our vectors we use the CountVectorizer that already removes punctuation and has an option to remove stopwords. When we are creating unigrams we are removing stop words, while for other ngrams we are keeping stopwords. The CountVectorizer is also used to create vectors for bigrams and trigrams. This can be done just by defining the ngram range within CountVectorizer: bigram = ngram_range=[2,2], trigram = ngram_range=[3,3].\n",
    "\n",
    "At the beginning of our modelling we will start with unigrams and later in the modelling part compare it to the results of using bigrams and trigrams.\n",
    "\n",
    "For these different types of vectors we intially wanted to create functions that take in a specific column of a data frame that is supposed to be vectorized (see below). However we decided to include this in our machine learning piplines right away. This is also important to consider for the later prediction phase. Since the test input vector has to have the same length as the train input vector not only the same vectorizer has to be used, the test text is reduced to only the n-grams that are also within the train vector. New n-grams from the test set would mean new features for the model on which it was never trained on and thus would not work. Thus the test set is later in the prediction phase vectorized using the same features as the train data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names()).drop(\"label\", axis=1) # hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(ngram_range=[2,2])\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(ngram_range=[3,3])\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification modelling\n",
    "For our machine learning process we will be working with four different algorithms:\n",
    "- __Max Entropy Classifier (Logistic Regression)__:\n",
    "_Probablistic classifier following a discriminative approach, uses the model with the maximum entropy (the probability distribution that represents the current state of knowledge the best is the one with largest entropy) - highly suited for text classification_\n",
    "\n",
    "- __Naive Bayes__: \n",
    "_Probablistic classifier based on the assumption of independence between the features, generative model - often used as baseline due to low computing times_\n",
    "\n",
    "- __Support Vector Machine Classifier__: \n",
    "_Non-probabilistic (binary linear) classifier following a discriminative approach - highly suited in high dimensional spaces (more features than samples), long computing time_\n",
    "\n",
    "- __Passive Aggressive__:\n",
    "_Online learning algorithm that performs \"passive\" if a sample was correctly classified and \"aggressive\" in cases of misclassification_\n",
    "\n",
    "In order to evaluate the performance of our models we will use 5-folds cross-validation focussing on the accuracy score ([TP+TN]/[TP+FP+TN+FN]). We will also include grid seach here in order to tune the hyperparameter of our algorithms.\n",
    "\n",
    "Throughout the process we are going to train these models with different features and then pick the best model among the trained ones based on the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Pipeline\n",
    "For convenience reasons we set up sum functions that will be used in the machine learning process. The grid_search_model function performs grid seach optimizing for the accuracy score of the defined k-fold cross validation score. We also drop in here all our text features when applying grid search since the texts will be vectorized before and we do not want to include these columns in our models.\n",
    "\n",
    "Furthermore all model crossvalidation results are saved within a dataset that shows the algorithm and features used as well as the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've had a bit of issues with memory errors, so clean our workspace of everything we don't need at the moment\n",
    "del train_data\n",
    "del train_data_clean\n",
    "del test_data\n",
    "del test_data_clean\n",
    "del test_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(data, model_meta, folds=5, drop=[]):\n",
    "    \"\"\"\n",
    "    Perform Grid Search Cross Validation on the input model\n",
    "\n",
    "    :param data: a pandas dataframe where each row is an hour\n",
    "    :param model_meta: An dict containing the name for the model (\"name\"), the sklearn estimator (\"model\"),\n",
    "                    and the parameters for Grid Search Cross Validation (\"params\")\n",
    "    :param folds: The number of splits for cross validation\n",
    "    :return: a tuple containing the best accuracy score found, the parameters used to obtain that score,\n",
    "                and the estimator retrained on the whole dataset\n",
    "    \"\"\"\n",
    "    model = model_meta[\"model\"]\n",
    "    model_params = model_meta[\"params\"]\n",
    "    model_name = model_meta[\"name\"]\n",
    "\n",
    "    to_drop = drop+[\"label\", \"index\", \"ID\", \"text\", \"title\", \"clean_text\", \"clean_title\", \"clean_combined\"]\n",
    "    X = data.drop(columns=to_drop)\n",
    "    y = data[\"label\"]\n",
    "\n",
    "    kfold = KFold(n_splits=folds)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_params, scoring=\"accuracy\", cv=kfold, refit=True)\n",
    "    \n",
    "    #print(\"Shape: {}\".format(y.iloc[0:1,]))\n",
    "    #print(X.head())\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"\\tAverage result for best {}: {} +/- {:.5f}\"\n",
    "          .format(model_name,\n",
    "                  grid_search.best_score_,\n",
    "                  grid_search.cv_results_[\"std_test_score\"][np.argmax(grid_search.cv_results_[\"mean_test_score\"])]))\n",
    "\n",
    "    print(\"\\tBest parameters for {0}: {1}\".format(model_name, grid_search.best_params_))\n",
    "\n",
    "    # Need metrics to choose model, best estimator will have already been retrained on whole data set\n",
    "    \n",
    "    retval = ({\n",
    "        \"model_name\": [model_name],\n",
    "        \"best_acc\": [grid_search.best_score_],\n",
    "        \"best_params\": [str(grid_search.best_params_)]\n",
    "    }, grid_search.best_estimator_)\n",
    "    \n",
    "    return retval    \n",
    "    \n",
    "    #return model_name, grid_search.best_score_, grid_search.best_params_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(result):\n",
    "    meta_info = result[0]\n",
    "    model = result[1]\n",
    "    \n",
    "    new_row = pd.DataFrame(meta_info)\n",
    "    model_results = pd.read_csv(\"models/model_results.csv\")\n",
    "    new_results = pd.concat([model_results, new_row], ignore_index=True)  #model_result.append(new_row)\n",
    "    new_results.to_csv(\"models/model_results.csv\", index=False)\n",
    "    \n",
    "    dump(model, \"models/{}.joblib\".format(meta_info[\"model_name\"][0]))\n",
    "         \n",
    "    return new_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Lexical features\n",
    "Our first approach is to only use our created lexical features including lexical diversity and the POS-tag distribution among title and text. No vectorized text is used for these models but only their lexical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Max Entropy Classifier (Logistic Regression) - Lexical Features\n",
    "We start here with the Max Entropy Classifier which represents a logistic regression. We see that the __accuracy is only at 57%__ and thus barely better than choosing randomly (50:50 distribution of target). Due to this poor performance we are not going to use the other three algorithms with only these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAverage result for best logit_no_vectorization: 0.5743935983995999 +/- 0.00883\n",
      "\tBest parameters for logit_no_vectorization: {'penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc        best_params\n",
       "0  logit_no_vectorization  0.574394  {'penalty': 'l2'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_no_vectorization = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_no_vectorization\",\n",
    "    \"params\": {\n",
    "        \"penalty\": [\"l1\", \"l2\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "save_result(grid_search_model(train_all_features, logit_no_vectorization))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Title unigrams\n",
    "As a next step we are using the vectorized title which create much less features than using the whole vectorized text. For all four models we achieve with this accuracy score between __77% (SVC, PA) and 80% (Logit, NB)__, while the __Naive Bayes algorithm performed best in this case__. Using such a rather simple algorithm on such little text produces already a significantly better outcome than classifying randomly. However, there is still a lot room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "In general, within the unigrams we still have STOPWORDS. I do not see that we have removed them since in all the unigrams steps we only include the ngram and not the input for stopwords! --> \"vectorizer\": CountVectorizer(ngram_range=[1,1])\n",
    "(See intial code below!)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names()).drop(\"label\", axis=1) # hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_grid_search(data, model_meta, folds=5, col=\"clean_combined\"):\n",
    "    model = model_meta[\"model\"]\n",
    "    model_params = model_meta[\"params\"]\n",
    "    model_name = model_meta[\"name\"]\n",
    "    vectorizer = model_meta[\"vectorizer\"]\n",
    "    \n",
    "    pipe = make_pipeline(vectorizer, model)\n",
    "\n",
    "    X = data[col]\n",
    "    y = data[\"label\"]\n",
    "\n",
    "    kfold = KFold(n_splits=folds)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=model_params, scoring=\"accuracy\", cv=kfold, refit=True)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"\\tAverage result for best {}: {} +/- {:.5f}\"\n",
    "          .format(model_name,\n",
    "                  grid_search.best_score_,\n",
    "                  grid_search.cv_results_[\"std_test_score\"][np.argmax(grid_search.cv_results_[\"mean_test_score\"])]))\n",
    "\n",
    "    print(\"\\tBest parameters for {0}: {1}\".format(model_name, grid_search.best_params_))\n",
    "\n",
    "    # Need metrics to choose model, best estimator will have already been retrained on whole data set\n",
    "    \n",
    "    retval = ({\n",
    "        \"model_name\": [model_name],\n",
    "        \"best_acc\": [grid_search.best_score_],\n",
    "        \"best_params\": [str(grid_search.best_params_)]\n",
    "    }, grid_search.best_estimator_)\n",
    "    \n",
    "    return retval    \n",
    "    \n",
    "    #return model_name, grid_search.best_score_, grid_search.best_params_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Max Entropy Classifier (Logistic Regression) - Title unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_title: 0.7991997999499875 +/- 0.01394\n",
      "\tBest parameters for logit_unigrams_title: {'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_title = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_unigrams_title\",\n",
    "    \"params\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              logit_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Naive Bayes - Title unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_unigrams_title: 0.8067016754188547 +/- 0.01153\n",
      "\tBest parameters for nb_unigrams_title: {'multinomialnb__alpha': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unigrams_title = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_unigrams_title\",\n",
    "    \"params\": {'multinomialnb__alpha': [0.5, 1, 2]},\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              nb_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Support Vector Machines - Title unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams_title: 0.7716929232308077 +/- 0.00985\n",
      "\tBest parameters for svc_unigrams_title: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams_title = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams_title\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              svc_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Passive aggressive - Title unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_title: 0.7754438609652413 +/- 0.01358\n",
      "\tBest parameters for pa_unigrams_title: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_title = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_unigrams_title\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              pa_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Unigrams\n",
    "The next approach is to not only use the title but title and text, which is included in our combined_text column. This column is then vectorized into unigrams by using the CountVectorizer which returns the actual count of a word within a text. Using the combined text __pushes up our accuracy scores to 88% (PA) to almost 91% (Logit)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Max Entropy Classifier (Logistic Regression) - Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams: 0.90847711927982 +/- 0.00840\n",
      "\tBest parameters for logit_unigrams: {'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_unigrams\",\n",
    "    \"params\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              logit_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Naive Bayes - Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_unigrams: 0.8877219304826206 +/- 0.00638\n",
      "\tBest parameters for nb_unigrams: {'multinomialnb__alpha': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unigrams = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_unigrams\",\n",
    "    \"params\": {'multinomialnb__alpha': [0.5, 1, 2]},\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              nb_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Support Vector Machine - Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams: 0.8924731182795699 +/- 0.00878\n",
      "\tBest parameters for svc_unigrams: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7            svc_unigrams  0.892473                                     {}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              svc_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Passive Aggressive - Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams: 0.8809702425606402 +/- 0.01259\n",
      "\tBest parameters for pa_unigrams: {'passiveaggressiveclassifier__C': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7            svc_unigrams  0.892473                                     {}\n",
       "8             pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_unigrams\",\n",
    "    \"params\": {\n",
    "        \"passiveaggressiveclassifier__C\": [0, 0.5, 1, 2]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              pa_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 TF-IDF\n",
    "\n",
    "As mentioned before when looking at vectorization we see that the vectors we create are weighted since they represent the occurence of words or bags of words. However, there are different approach in how to represent weights (binary, absolute frequency, normalized). TF-IDF combines the idea of term frequency (TF - frequency in a document) and inverse document frequency (IDF - rareness in a collection) and creates a weight for each word/bag of words building the product of term frequency and inverse document frequency. \n",
    "\n",
    "Overall, using a __TF-IDF weight improves the accuracy for MaxEnt, SVM and PA__ but decreases the accuracy for Naive Bayes. In particular the SVM result increased significantly from 89.3% to 92.8%, which is currently our best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Max Entropy Classifier (Logistic Regression) - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_tfidf: 0.9099774943735934 +/- 0.00578\n",
      "\tBest parameters for logit_unigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7            svc_unigrams  0.892473                                     {}\n",
       "8             pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9    logit_unigrams_tfidf  0.909977                                     {}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_tfidf = {\n",
    "    \"model\": LogisticRegression(penalty=\"l2\"),\n",
    "    \"name\": \"logit_unigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              logit_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Naive Bayes - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_unigrams_tfidf: 0.8559639909977494 +/- 0.01136\n",
      "\tBest parameters for nb_unigrams_tfidf: {'multinomialnb__alpha': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  best_acc                            best_params\n",
       "0   logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1     logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2        nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3       svc_unigrams_title  0.771693                                     {}\n",
       "4        pa_unigrams_title  0.775444                                     {}\n",
       "5           logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6              nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7             svc_unigrams  0.892473                                     {}\n",
       "8              pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9     logit_unigrams_tfidf  0.909977                                     {}\n",
       "10       nb_unigrams_tfidf  0.855964          {'multinomialnb__alpha': 0.5}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unigrams_tfidf = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_unigrams_tfidf\",\n",
    "    \"params\": {'multinomialnb__alpha': [0.5, 1, 2]},\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              nb_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 Support Vector Machine - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams_tfidf: 0.9287321830457614 +/- 0.00670\n",
      "\tBest parameters for svc_unigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  best_acc                            best_params\n",
       "0   logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1     logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2        nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3       svc_unigrams_title  0.771693                                     {}\n",
       "4        pa_unigrams_title  0.775444                                     {}\n",
       "5           logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6              nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7             svc_unigrams  0.892473                                     {}\n",
       "8              pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9     logit_unigrams_tfidf  0.909977                                     {}\n",
       "10       nb_unigrams_tfidf  0.855964          {'multinomialnb__alpha': 0.5}\n",
       "11      svc_unigrams_tfidf  0.928732                                     {}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams_tfidf = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4 Passive Aggressive - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_tfidf: 0.8927231807951987 +/- 0.00863\n",
      "\tBest parameters for pa_unigrams_tfidf: {'passiveaggressiveclassifier__C': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  best_acc                            best_params\n",
       "0   logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1     logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2        nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3       svc_unigrams_title  0.771693                                     {}\n",
       "4        pa_unigrams_title  0.775444                                     {}\n",
       "5           logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6              nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7             svc_unigrams  0.892473                                     {}\n",
       "8              pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9     logit_unigrams_tfidf  0.909977                                     {}\n",
       "10       nb_unigrams_tfidf  0.855964          {'multinomialnb__alpha': 0.5}\n",
       "11      svc_unigrams_tfidf  0.928732                                     {}\n",
       "12       pa_unigrams_tfidf  0.892723  {'passiveaggressiveclassifier__C': 1}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_unigrams_tfidf\",\n",
    "    \"params\": {\n",
    "        \"passiveaggressiveclassifier__C\": [0, 0.5, 1, 2]\n",
    "    },\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              pa_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 TF-IDF plus lexical features\n",
    "Since our previously created lexical features performed comparatively poor by their own, we are tying to combine this information with the vectorized text. Thus we are including these features together with the TF-IDF weighted unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df, col=\"clean_combined\"):\n",
    "    vec = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "    X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "    return pd.DataFrame(X.toarray(), columns=vec.get_feature_names()).drop(\"label\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building unigram tfidf frame...\n"
     ]
    }
   ],
   "source": [
    "# We have had issues with memory, so using our tf_idf which converts the \n",
    "# scipy sparse matrix to a dense numpy array is not going to help. But this is  the only way we know how\n",
    "# to combine the scipy matrix from the vectorizer and our dataframe of engineered features.\n",
    "# so if we get a huge spike in performance we'll continue, but otherwise we will just move on to. \n",
    "\n",
    "print(\"Building unigram tfidf frame...\")\n",
    "unigram_frame_tfidf = pd.concat([train_all_features,\n",
    "                                 tf_idf(train_all_features, col=\"clean_combined\")],\n",
    "                                axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1 Max Entropy Classifier (Logistic Regression) - TF-IDF plus lexical features\n",
    "Including these features actually __worsens our accuracy score for the logistic regression__ compared to the model that only uses TF-IDF. Thus we will not continue to include out features since they do not seem to add any explaining value and also due to time reasons because of the problem to concatinate the features with the sparse vector matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_tfidf_all_features: 0.9094773693423356 +/- 0.00412\n",
      "\tBest parameters for logit_unigrams_tfidf_all_features: {'penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_tfidf_all_features = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_unigrams_tfidf_all_features\",\n",
    "    \"params\": {\n",
    "        \"penalty\": [\"l1\", \"l2\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(grid_search_model(unigram_frame_tfidf,\n",
    "                              logit_unigrams_tfidf_all_features,))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Bigrams\n",
    "Our next step is to use bigrams (bags of two words) instead of unigrams. This increases our number of features significantly (and thus computing time) but bigrams are able to capture local dependencies within a text and the order of words. Thus entities and relations between words can be captured even though using n-grams in general produce many meaningless n-grams due to the increased vocabulary size (features).\n",
    "\n",
    "Using __bigrams instead of solely unigrams increases the accuracy for all four models__. The only model that has a lower accuracy with bigrams compared to TF-IDF weighted unigrams is the SVM model. Thus in a next step we will apply TF-IDF for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.1 Max Entropy Classifier (Logistic Regression) - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_bigrams: 0.9102275568892223 +/- 0.00670\n",
      "\tBest parameters for logit_bigrams: {'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_bigrams = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_bigrams\",\n",
    "    \"params\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, logit_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.2 Naive Bayes - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_bigrams: 0.8924731182795699 +/- 0.00583\n",
      "\tBest parameters for nb_bigrams: {'multinomialnb__alpha': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_bigrams = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_bigrams\",\n",
    "    \"params\": {\n",
    "        \"multinomialnb__alpha\": [0, 0.5, 1]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, nb_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.2 Support Vector Machine - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_bigrams: 0.8939734933733433 +/- 0.00898\n",
      "\tBest parameters for svc_bigrams: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_bigrams = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_bigrams\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.4 Passive Aggressive - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_bigrams: 0.9034758689672419 +/- 0.00632\n",
      "\tBest parameters for pa_bigrams: {'passiveaggressiveclassifier__C': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_bigrams = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_bigrams\",\n",
    "    \"params\": {\n",
    "        \"passiveaggressiveclassifier__C\": [0, 0.5, 1, 2]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Bigrams and TF-IDF\n",
    "Using TF-IDF weight on bigrams improves our model for SVM slightly compared to TF-IDF weighted unigrams, but pushes the accuracy of PA significantly up to 92.9%. For time reasons we decided not to use the MaxEnt and the Naive Bayes classifier in this case.\n",
    "\n",
    "Overall __TF-IDF weights seem to make a true difference in the performance__ of our classification models. Thus we will continue to use them in the following models instead of the usual CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7.1 Support Vector Machine - Bigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_bigrams_tfidf: 0.9294823705926482 +/- 0.00883\n",
      "\tBest parameters for svc_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_bigrams_tfidf = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7.2 Passive Agressive - Bigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_bigrams_tfidf: 0.929732433108277 +/- 0.01153\n",
      "\tBest parameters for pa_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_bigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Unigrams, Bigrams and TF-IDF\n",
    "Since we saw that some models perform better with bigrams than with unigrams, we are going to combine both approaches in the next step. Using ngram_range=(1,2) allows us to use unigrams and bigrams within our vector space model. As mentioned before we continue to use the TF-IDF weights here.\n",
    "\n",
    "While for __MaxEnt the accuracy drops, SVM and PA produce accuracies above 93.5%__. Thus our best model so far is the Passive Aggressive algorithm with an accuracy of 93.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8.1 Max Entropy Classifier (Logistic Regression) - Unigrams, bigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_bigrams_tfidf: 0.8989747436859215 +/- 0.00790\n",
      "\tBest parameters for logit_unigrams_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_bigrams_tfidf = {\n",
    "    \"model\": LogisticRegression(penalty='l2'), # seems to do the best,trying to save tim\n",
    "    \"name\": \"logit_unigrams_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, logit_unigrams_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8.2 Support Vector Machine - Unigrams, bigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams_bigrams_tfidf: 0.9364841210302576 +/- 0.00783\n",
      "\tBest parameters for svc_unigrams_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams_bigrams_tfidf = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_unigrams_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8.3 Passive Aggressive - Unigrams, bigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_bigrams_tfidf: 0.9389847461865466 +/- 0.00848\n",
      "\tBest parameters for pa_unigrams_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22          pa_unigrams_bigrams_tfidf  0.938985   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_bigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_unigrams_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_unigrams_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Trigrams and TF-IDF\n",
    "Our next step is the use of trigrams which create tree bags of three words based on our combined text.\n",
    "\n",
    "Similar to the unigram and bigram combination we continue to use TF-IDF for trigrams since including it constantly outperformed the respective models of unigrams and bigrams that were not using TF-IDF.\n",
    "\n",
    "Since we saw that the passive aggressive algorithm tends to perform best, we are starting with this model. However, due to the fact that __the accuracy for this model significantly drops (90%)__ compared to the result for bigrams and TF-IDF (93.9%), we will not continue to test trigrams in combination of only TF-IDF for the other three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.9.1 Passive Aggressive - Trigrams and TDF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_trigrams_tfidf: 0.9019754938734683 +/- 0.01288\n",
      "\tBest parameters for pa_trigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pa_trigrams_tfidf</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22          pa_unigrams_bigrams_tfidf  0.938985   \n",
       "23                  pa_trigrams_tfidf  0.901975   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  \n",
       "23                                     {}  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_trigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), \n",
    "    \"name\": \"pa_trigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(3,3))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_trigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 Bigrams, trigrams and TF-IDF\n",
    "Since combining unigrams and bigrams (plus TF-IDF) increased our accuracy for SVM and Passive Aggresive we starting now with a Passive Aggresive model with bigrams, trigrams and TF-IDF. Unfortunatly __the accuracy is on 92.3% compared to the 93.9% of using unigrams instead of trigrams__. Thus we will not continue to evaluate model for this combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.10.1 Passive Aggresive - Bigrams, trigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_bigrams_trigrams_tfidf: 0.9234808702175544 +/- 0.00853\n",
      "\tBest parameters for pa_bigrams_trigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pa_trigrams_tfidf</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pa_bigrams_trigrams_tfidf</td>\n",
       "      <td>0.923481</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22          pa_unigrams_bigrams_tfidf  0.938985   \n",
       "23                  pa_trigrams_tfidf  0.901975   \n",
       "24          pa_bigrams_trigrams_tfidf  0.923481   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  \n",
       "23                                     {}  \n",
       "24                                     {}  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_bigrams_trigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_bigrams_trigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(2,3))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_bigrams_trigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 Unigrams, bigrams, trigrams and TF-IDF\n",
    "The last model combines now all created n-grams as well as TF-IDF. Again we start with Passive Aggressive and will not continue with other models if the accuracy does not improve.\n",
    "\n",
    "Overall, we see that the __accuracy score drops down to 93.1%__ compared to our best model without the trigrams (93.9%). Thus we will not continue trying this feature combination for the other three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.11.1 Passive Aggressive - Unigrams, bigrams, trigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_bigrams_trigrams_tfidf: 0.9314828707176794 +/- 0.01050\n",
      "\tBest parameters for pa_unigrams_bigrams_trigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pa_trigrams_tfidf</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pa_bigrams_trigrams_tfidf</td>\n",
       "      <td>0.923481</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pa_unigrams_bigrams_trigrams_tfidf</td>\n",
       "      <td>0.931483</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model_name  best_acc  \\\n",
       "0               logit_no_vectorization  0.574394   \n",
       "1                 logit_unigrams_title  0.799200   \n",
       "2                    nb_unigrams_title  0.806702   \n",
       "3                   svc_unigrams_title  0.771693   \n",
       "4                    pa_unigrams_title  0.775444   \n",
       "5                       logit_unigrams  0.908477   \n",
       "6                          nb_unigrams  0.887722   \n",
       "7                         svc_unigrams  0.892473   \n",
       "8                          pa_unigrams  0.880970   \n",
       "9                 logit_unigrams_tfidf  0.909977   \n",
       "10                   nb_unigrams_tfidf  0.855964   \n",
       "11                  svc_unigrams_tfidf  0.928732   \n",
       "12                   pa_unigrams_tfidf  0.892723   \n",
       "13   logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                       logit_bigrams  0.910228   \n",
       "15                          nb_bigrams  0.892473   \n",
       "16                         svc_bigrams  0.893973   \n",
       "17                          pa_bigrams  0.903476   \n",
       "18                   svc_bigrams_tfidf  0.929482   \n",
       "19                    pa_bigrams_tfidf  0.929732   \n",
       "20        logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21          svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22           pa_unigrams_bigrams_tfidf  0.938985   \n",
       "23                   pa_trigrams_tfidf  0.901975   \n",
       "24           pa_bigrams_trigrams_tfidf  0.923481   \n",
       "25  pa_unigrams_bigrams_trigrams_tfidf  0.931483   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  \n",
       "23                                     {}  \n",
       "24                                     {}  \n",
       "25                                     {}  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_bigrams_trigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2),\n",
    "    \"name\": \"pa_unigrams_bigrams_trigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,3))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_unigrams_bigrams_trigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final model\n",
    "Our final model is the one with the highest cross-validation accuracy. In our case this is the __Passive Aggressive algorithm using unigrams, bigrams and TF-IDF weights__ as input. This model led to an accuracy score of 93.9% in our 5-fold crossvaldation. We are now excessing the saved model and are using this to make predictions on our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth...=None, shuffle=True, tol=None,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"models/model_results.csv\")\n",
    "best_model = results[results[\"best_acc\"] == max(results[\"best_acc\"])][\"model_name\"].iloc[0]\n",
    "\n",
    "final_model = load(\"models/{}.joblib\".format(best_model))\n",
    "\n",
    "final_model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model produces a very __similar distribution of real and false classified texts__ as we have in the train data set (almost 50:50). It is important to check the predicte target distribution because if it is very different then there could be a problem with the data or the test data set is just very different from the actual train data set. However, in our case the distributions are very similar and we can go on and save our __predictions with the corresponding IDs in a csv file (can be found in data folder)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REAL    1162\n",
       "FAKE    1159\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_features = pd.read_csv(\"data/test_all_features.csv\")\n",
    "preds = pd.Series(final_model.predict(test_all_features[\"clean_combined\"])).rename(\"prediction\")\n",
    "\n",
    "preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8430</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1220</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9624</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8211</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4099</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>878</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5304</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>92</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1545</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3517</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10414</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6109</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6456</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7431</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9078</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8054</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5252</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>219</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10223</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6802</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6951</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3003</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6855</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3548</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2050</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>6457</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>7030</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>9013</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>9509</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>3825</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>4515</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>2747</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>6516</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>9636</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>7398</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>3717</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>5205</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>6696</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>7991</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>1303</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>9051</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>10200</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>10009</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>4214</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>2316</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>8411</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>6143</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>3262</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>9337</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>8737</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>4490</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>8062</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>8622</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>4021</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>4330</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2321 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID prediction\n",
       "0     10498       FAKE\n",
       "1      2439       REAL\n",
       "2       864       REAL\n",
       "3      4128       REAL\n",
       "4       662       REAL\n",
       "5      8430       FAKE\n",
       "6      1220       REAL\n",
       "7      9624       FAKE\n",
       "8      8211       FAKE\n",
       "9      4099       REAL\n",
       "10      878       REAL\n",
       "11     5304       FAKE\n",
       "12       92       REAL\n",
       "13     1545       REAL\n",
       "14     3517       REAL\n",
       "15    10414       FAKE\n",
       "16     6109       FAKE\n",
       "17     6456       FAKE\n",
       "18     7431       REAL\n",
       "19     9078       FAKE\n",
       "20     8054       REAL\n",
       "21     5252       REAL\n",
       "22      219       REAL\n",
       "23    10223       FAKE\n",
       "24     6802       FAKE\n",
       "25     6951       REAL\n",
       "26     3003       REAL\n",
       "27     6855       FAKE\n",
       "28     3548       REAL\n",
       "29     2050       REAL\n",
       "...     ...        ...\n",
       "2291   6457       REAL\n",
       "2292   7030       FAKE\n",
       "2293   9013       FAKE\n",
       "2294   9509       FAKE\n",
       "2295   3825       REAL\n",
       "2296   4515       REAL\n",
       "2297   2747       REAL\n",
       "2298   6516       FAKE\n",
       "2299   9636       REAL\n",
       "2300   7398       FAKE\n",
       "2301   3717       REAL\n",
       "2302   5205       REAL\n",
       "2303   6696       FAKE\n",
       "2304   7991       FAKE\n",
       "2305   1303       REAL\n",
       "2306   9051       FAKE\n",
       "2307  10200       FAKE\n",
       "2308  10009       FAKE\n",
       "2309   4214       REAL\n",
       "2310   2316       REAL\n",
       "2311   8411       FAKE\n",
       "2312   6143       FAKE\n",
       "2313   3262       REAL\n",
       "2314   9337       FAKE\n",
       "2315   8737       FAKE\n",
       "2316   4490       REAL\n",
       "2317   8062       FAKE\n",
       "2318   8622       FAKE\n",
       "2319   4021       REAL\n",
       "2320   4330       REAL\n",
       "\n",
       "[2321 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([test_all_features[\"ID\"], preds], axis=1).to_csv(\"data/predictions.csv\",index=False)\n",
    "pd.read_csv(\"data/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "In order to see how well our model is performing we create a random 20% test split. Eventhough this is similar to cross validation (just with only one fold) with this procedure we can actually set up a confusion matix for our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_copy = load(\"models/{}.joblib\".format(best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred   obs\n",
       "0   REAL  REAL\n",
       "1   REAL  REAL\n",
       "2   REAL  REAL\n",
       "3   FAKE  FAKE\n",
       "4   REAL  REAL\n",
       "5   REAL  FAKE\n",
       "6   FAKE  FAKE\n",
       "7   REAL  REAL\n",
       "8   FAKE  FAKE\n",
       "9   REAL  REAL\n",
       "10  REAL  REAL\n",
       "11  FAKE  FAKE\n",
       "12  REAL  FAKE\n",
       "13  FAKE  FAKE\n",
       "14  REAL  REAL\n",
       "15  REAL  FAKE\n",
       "16  REAL  FAKE\n",
       "17  REAL  REAL\n",
       "18  REAL  REAL\n",
       "19  REAL  FAKE"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(data, model):\n",
    "    X = data[\"clean_combined\"]\n",
    "    y = data[\"label\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=20193105)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = pd.Series(model.predict(X_test)).rename(\"pred\")\n",
    "    obs = y_test.reset_index()[\"label\"].rename(\"obs\")\n",
    "    \n",
    "    return pd.concat([preds, obs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_results = evaluate_model(train_all_features, final_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 394, 28)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_tp = lambda row: row[\"pred\"] == \"FAKE\" and row[\"obs\"] == \"FAKE\"\n",
    "is_fp = lambda row: row[\"pred\"] == \"FAKE\" and row[\"obs\"] == \"REAL\"\n",
    "is_tn = lambda row: row[\"pred\"] == \"REAL\" and row[\"obs\"] == \"REAL\"\n",
    "is_fn = lambda row: row[\"pred\"] == \"REAL\" and row[\"obs\"] == \"FAKE\"\n",
    "\n",
    "TP = sum(holdout_results.apply(is_tp, axis=1))\n",
    "FP = sum(holdout_results.apply(is_fp, axis=1))\n",
    "TN = sum(holdout_results.apply(is_tn, axis=1))\n",
    "FN = sum(holdout_results.apply(is_fn, axis=1))\n",
    "\n",
    "(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25bbe752a58>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "#from matplotlib.colors import r, g\n",
    "df_cm = pd.DataFrame([[TP, FP], \n",
    "                     [FN, TN]],\n",
    "                     index = [i for i in [\"FAKE\", \"REAL\"]],\n",
    "                     columns = [i for i in [\"FAKE\", \"REAL\"]])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\".0f\", cbar=False, annot_kws={\"size\": 20}, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an __approximate accuracy 0.93__ which is a bit lower than our previous cross-validation scores. However, the model shows the exact count for False Positive as well as False Negatives for this hold out and we are able to print a confusion matrix to see the actual errors (FP, FN). Since FP as well as FN are at 28 for this hold out we are almost equally well predicting for real as well as for fake articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "After testing 25 different models we chose to go with the Passive Aggressive algorithm using unigrams and bigrams as well as TF-IDF weighting. This models seems to produce __accuracy results between 93% and 94%__, which is an almost perfect classification. The performance on our test data set however might decrease due to the fact the test vectorizer is using the features of our train vectorizer. Thus some unigrams and bigrams that can be constructed from our test text might not be included in the used vectorizer for predicting since these words simply do not appear in the train data set. Thus such classifiers need to be constantly trained with new data in order to perform well on data that it has never seen before.\n",
    "\n",
    "Overall in the process of our machine learning modelling we see that some algorithms perform better with certain features that other. It does not seem that there is superior algorithm or feature combination since the accuracy scores differ among algorithms and feature selection. Nevertheless, the strategy to use TF-IDF weighting seems to outperform the more simple CountVectorizer approach. One further reason why we chose to go with the Passive Aggressive algorithm instead of the SVM classifier that produced a similar high result is computing time. SVM tend to take very long time and thus are more constly when implementing. In additon, __to stop the spread of fake news time is crucial__. Facebook is constantly criticised for being too slow in flagging or deleting fake news on their platform. Consequently a faster performing algorithm can make quiet a difference when it comes to fighting fake news in real life.\n",
    "\n",
    "Eventhough we tested more than 20 different models there is always something else that can be tried or optimized. Thus as further steps we would start to perform __feature selection__ as well as trying __different lemmatizers__. Especially feature selection would help to focus more on the ones that seem to have a higher importance.\n",
    "\n",
    "Nevertheless, we are for our first NLP project quiet happy with this model, which would actually help us to not get blinded anymore by fake news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
