{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Lanuguage Processing\n",
    "## Fake news identification\n",
    "\n",
    "by Daniel Russotto and Christine Utendorf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment our goal is to determine if a provided article including a title and text provides real or fake news. Fake news consists of disinformation and it imposes a great threat to our today's society. Not knowing what to believe when it comes to news and information or even worse not recognizing that the information provided is not reflecting the truth can truely harm a reader. Especially through the rise of the internet and with it the rise of social media, news can be accessed at any time, any place and from many different sources. However, this also gives fake news the possibility to spread faster, wider and more successfully.\n",
    "\n",
    "Social media giant Facebook has set up a unit to identify such fake news on its platform. After being critized more than ones for doing to little against the spread of false infromation, Facebook is now \"working to stop misinformation and false news\". The company is not only working together with third-party fact-check organizations but is also applying machine learning techniques to identify such post that contain fake news (see more under the [link](https://www.facebook.com/facebookmedia/blog/working-to-stop-misinformation-and-false-news)). It is highly likely that Facebook uses Natural Language Processing and classification algorithms in order to determine if they have fraud in front of them or not.\n",
    "\n",
    "In this assignment we (Dan and Christine) are going to work on such a problem that Facebook (as well as Twitter, Youtube, and many other platforms) is facing everyday: Identifying is an article provides real or fake news. We are not going to use fact-checking in order to prove if an information is acurate, but train a machine learning algorithm to classify articles as fake or real. In order to do so we are using several concepts of natural language processing such as tokenization and lemmatization from the NLTK python library as well as machine learning concepts including logistic regression and naive bayes from the sklearn python library. For this puprpose we were provided with a training data set that includes articels that are already labled as real or fake and a test set without such labels. The goal is to train a model that is able to find a general pattern to identify fake news among articles it has never seen before (here our \"blind\" test data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import sklearn\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from functools import reduce\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/fake_or_real_news_training.csv\", quotechar='\"', header=0, sep=\",\",\n",
    "                    index_col=\"ID\", encoding='utf-8')\n",
    "test_data = pd.read_csv(\"data/fake_or_real_news_test.csv\", quotechar='\"', header=0, sep=\",\",\n",
    "                   index_col=\"ID\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set\n",
    "The training data set has 3,999 rows each representing an article. All articles are identified with a unique id, have a title, text and a label if they are fake or real. Furthermore there are the columns X1 and X2. These two columns should actually be all filled with NaN (=empty). However, 33 rows show values in X1 and 2 of these 33 also in X2. This shows that the text was not properly splitted in these cases. The separator used to splitt the csv into a dataframe is \",\" but as it seems in some of the cases this did not split all rows correctly. In the data cleaning part we will take a closer look at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3999 entries, 8476 to 9673\n",
      "Data columns (total 5 columns):\n",
      "title    3999 non-null object\n",
      "text     3999 non-null object\n",
      "label    3999 non-null object\n",
      "X1       33 non-null object\n",
      "X2       2 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 187.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label   X1   X2  \n",
       "ID                                                                        \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "875    It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data\n",
    "The test data set consists of 2321 unlabeled rows and due to the fact that the data frame only includes the unique id, the title and the actual text it seems as the text split worked well here (no X1 or X2). The train data has less than double the amount of articles in it compared to the test data set. This makes it crucial to retrain at the end the machine learning with the complete train data set due to limited data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2321 entries, 10498 to 4330\n",
      "Data columns (total 2 columns):\n",
      "title    2321 non-null object\n",
      "text     2321 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 54.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                    text  \n",
       "ID                                                        \n",
       "10498  September New Homes Sales Rise Back To 1992 Le...  \n",
       "2439   But when Congress debated and passed the Patie...  \n",
       "864    The Bernie Sanders and Ted Cruz campaigns vowe...  \n",
       "4128   Police searching for the second of two escaped...  \n",
       "662    No matter who wins California's 475 delegates ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data cleaning\n",
    "As seen in the train_data.info() we have several rows that were not correctly put in the dataframe. We are now going to fix these rows. The csv is splitted using commas. Thus a title or text that has commas in it is splitted incorrectly. A first step is to filter out the rows that are displaced and take a closer look at them. Overall, we have 33 rows with displaced values since all the rows that have values in X2 have values in X1. It is important to fix these values due to the limited number of training rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Election Day: No Legal Pot In Ohio</td>\n",
       "      <td>Democrats Lose In The South</td>\n",
       "      <td>Election Day: No Legal Pot In Ohio; Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194</th>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>Leonardo DiCaprio to the rescue?</td>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Black Hawk crashes off Florida</td>\n",
       "      <td>human remains found</td>\n",
       "      <td>(CNN) Thick fog forced authorities to suspend ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital</td>\n",
       "      <td>U.S. investigating</td>\n",
       "      <td>(CNN) Aerial bombardments blew apart a Doctors...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>US issues travel warning</td>\n",
       "      <td>A member of Al Qaeda's branch in Yemen said Fr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>Shallow 5.4 magnitude earthquake rattles centr...</td>\n",
       "      <td>shakes buildings in Rome</td>\n",
       "      <td>00 UTC © USGS Map of the earthquake's epicent...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>ICE Agent Commits Suicide in NYC</td>\n",
       "      <td>Leaves Note Revealing Gov’t Plans to Round-up...</td>\n",
       "      <td>Email Print After writing a lengthy suicide no...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9203</th>\n",
       "      <td>Political Correctness for Yuengling Brewery</td>\n",
       "      <td>What About Our Opioid Epidemic?</td>\n",
       "      <td>We Are Change \\r\\n\\r\\nIn today’s political cli...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>Poll gives Biden edge over Clinton against GOP...</td>\n",
       "      <td>VP meets with Trumka</td>\n",
       "      <td>A new national poll shows Vice President Biden...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>Russia begins airstrikes in Syria</td>\n",
       "      <td>U.S. warns of new concerns in conflict</td>\n",
       "      <td>Russian warplanes began airstrikes in Syria on...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>Trump &amp;amp</td>\n",
       "      <td>Clinton Were Very Convincing...on How Lousy t...</td>\n",
       "      <td>Let's pretend for a moment that the biggest he...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>Belgian police mount raids</td>\n",
       "      <td>prosecutors acknowledge missed opportunities</td>\n",
       "      <td>Belgian authorities missed a chance to press a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...</td>\n",
       "      <td>GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS</td>\n",
       "      <td>Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues</td>\n",
       "      <td>Brothers Were On No-Fly List</td>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues;...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>The Amish In America Commit Their Vote To Dona...</td>\n",
       "      <td>Mathematically Guaranteeing Him A Presidentia...</td>\n",
       "      <td>18 SHARE The Amish in America have committed t...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6404</th>\n",
       "      <td>#BREAKING: SECOND Assassination Attempt On Tru...</td>\n",
       "      <td>Suspect Detained (LIVE BLOG)</td>\n",
       "      <td>We Are Change \\r\\nDonald Trump on Saturday was...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>30th Infantry Division: “Work Horse of the Wes...</td>\n",
       "      <td>The Big Picture TV-211</td>\n",
       "      <td>Published on Oct 27, 2016 by Jeff Quitney The ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Planned Parenthood’s lobbying effort</td>\n",
       "      <td>pay raises for federal workers</td>\n",
       "      <td>and the future Fed rates</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10492</th>\n",
       "      <td>TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...</td>\n",
       "      <td>“THE END OF LIFE AS WE KNOW IT”</td>\n",
       "      <td>Paul Joseph Watson Senior British army officer...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>Inside The Mind Of An FBI Informant</td>\n",
       "      <td>Terri Linnell Admits Role As Gov’t Snitch</td>\n",
       "      <td>Inside The Mind Of An FBI Informant; Terri Lin...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>Gary Johnson Avoids Typical Third-Party Fade</td>\n",
       "      <td>Best Polling Since Perot in ‘92</td>\n",
       "      <td>A couple of weeks ago in this space I pushed b...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Nearly 300K New Jobs In February</td>\n",
       "      <td>Unemployment Dips To 5.5 Percent</td>\n",
       "      <td>Nearly 300K New Jobs In February; Unemployment...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>Why Trump Won</td>\n",
       "      <td>Why Clinton Lost</td>\n",
       "      <td>WashingtonsBlog \\r\\nBy Robert Parry, the inv...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>Jesse Matthew charged in Hannah Graham's murder</td>\n",
       "      <td>DA will not pursue death penalty</td>\n",
       "      <td>Jesse Matthew Jr., a former hospital worker, w...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8748</th>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRiot</td>\n",
       "      <td>Media Ignores (Video)</td>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRio...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>Jim Rogers: It’s Time To Prepare</td>\n",
       "      <td>Economic And Financial Collapse Imminent (VIDEO)</td>\n",
       "      <td>By: The Voice of Reason | Regardless of how mu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Islamic State admits defeat in Kobani</td>\n",
       "      <td>blames airstrikes</td>\n",
       "      <td>Islamic State militants have acknowledged for ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>Clinton Cries Racism Tagging Trump with KKK</td>\n",
       "      <td>Trump Says 'She Lies'</td>\n",
       "      <td>With only about 70 days left until the electio...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Suspects In Paris Magazine Attack Killed</td>\n",
       "      <td>Market Gunman And 4 Hostages Also Dead</td>\n",
       "      <td>Suspects In Paris Magazine Attack Killed; Mark...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>Chart Of The Day: Since 2009 Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>Ted Cruz launches bid</td>\n",
       "      <td>Some pundits paint him as scary extremist</td>\n",
       "      <td>Before he got to repealing ObamaCare, before h...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>State Dept. IDs 2 Americans killed in Nepal quake</td>\n",
       "      <td>2 others reportedly dead</td>\n",
       "      <td>The State Department identified two Americans ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>bursting of firecrackers suspected</td>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "599                   Election Day: No Legal Pot In Ohio   \n",
       "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "356                       Black Hawk crashes off Florida   \n",
       "2786      Afghanistan: 19 die in air attacks on hospital   \n",
       "3622   Al Qaeda rep says group directed Paris magazin...   \n",
       "7375   Shallow 5.4 magnitude earthquake rattles centr...   \n",
       "9097                    ICE Agent Commits Suicide in NYC   \n",
       "9203         Political Correctness for Yuengling Brewery   \n",
       "1602   Poll gives Biden edge over Clinton against GOP...   \n",
       "4562                   Russia begins airstrikes in Syria   \n",
       "4748                                          Trump &amp   \n",
       "3508                          Belgian police mount raids   \n",
       "7559   STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...   \n",
       "3634       The Latest On Paris Attack: Manhunt Continues   \n",
       "8470   The Amish In America Commit Their Vote To Dona...   \n",
       "6404   #BREAKING: SECOND Assassination Attempt On Tru...   \n",
       "10499  30th Infantry Division: “Work Horse of the Wes...   \n",
       "9                   Planned Parenthood’s lobbying effort   \n",
       "10492  TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...   \n",
       "10138                Inside The Mind Of An FBI Informant   \n",
       "4953        Gary Johnson Avoids Typical Third-Party Fade   \n",
       "496                     Nearly 300K New Jobs In February   \n",
       "5741                                       Why Trump Won   \n",
       "4131     Jesse Matthew charged in Hannah Graham's murder   \n",
       "8748       WATCH: Mass Shooting Occurs During #TrumpRiot   \n",
       "6717                    Jim Rogers: It’s Time To Prepare   \n",
       "2943               Islamic State admits defeat in Kobani   \n",
       "5248         Clinton Cries Racism Tagging Trump with KKK   \n",
       "3624            Suspects In Paris Magazine Attack Killed   \n",
       "6268   Chart Of The Day: Since 2009—–Recovery For The 5%   \n",
       "2738                               Ted Cruz launches bid   \n",
       "4025   State Dept. IDs 2 Americans killed in Nepal quake   \n",
       "9954   Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "599                          Democrats Lose In The South   \n",
       "10194                   Leonardo DiCaprio to the rescue?   \n",
       "356                                  human remains found   \n",
       "2786                                  U.S. investigating   \n",
       "3622                            US issues travel warning   \n",
       "7375                            shakes buildings in Rome   \n",
       "9097    Leaves Note Revealing Gov’t Plans to Round-up...   \n",
       "9203                     What About Our Opioid Epidemic?   \n",
       "1602                                VP meets with Trumka   \n",
       "4562              U.S. warns of new concerns in conflict   \n",
       "4748    Clinton Were Very Convincing...on How Lousy t...   \n",
       "3508        prosecutors acknowledge missed opportunities   \n",
       "7559        GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS   \n",
       "3634                        Brothers Were On No-Fly List   \n",
       "8470    Mathematically Guaranteeing Him A Presidentia...   \n",
       "6404                        Suspect Detained (LIVE BLOG)   \n",
       "10499                             The Big Picture TV-211   \n",
       "9                         pay raises for federal workers   \n",
       "10492                    “THE END OF LIFE AS WE KNOW IT”   \n",
       "10138          Terri Linnell Admits Role As Gov’t Snitch   \n",
       "4953                     Best Polling Since Perot in ‘92   \n",
       "496                     Unemployment Dips To 5.5 Percent   \n",
       "5741                                    Why Clinton Lost   \n",
       "4131                    DA will not pursue death penalty   \n",
       "8748                               Media Ignores (Video)   \n",
       "6717    Economic And Financial Collapse Imminent (VIDEO)   \n",
       "2943                                   blames airstrikes   \n",
       "5248                               Trump Says 'She Lies'   \n",
       "3624              Market Gunman And 4 Hostages Also Dead   \n",
       "6268                              Stagnation for the 95%   \n",
       "2738           Some pundits paint him as scary extremist   \n",
       "4025                            2 others reportedly dead   \n",
       "9954                  bursting of firecrackers suspected   \n",
       "\n",
       "                                                   label  \\\n",
       "ID                                                         \n",
       "599    Election Day: No Legal Pot In Ohio; Democrats ...   \n",
       "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "356    (CNN) Thick fog forced authorities to suspend ...   \n",
       "2786   (CNN) Aerial bombardments blew apart a Doctors...   \n",
       "3622   A member of Al Qaeda's branch in Yemen said Fr...   \n",
       "7375    00 UTC © USGS Map of the earthquake's epicent...   \n",
       "9097   Email Print After writing a lengthy suicide no...   \n",
       "9203   We Are Change \\r\\n\\r\\nIn today’s political cli...   \n",
       "1602   A new national poll shows Vice President Biden...   \n",
       "4562   Russian warplanes began airstrikes in Syria on...   \n",
       "4748   Let's pretend for a moment that the biggest he...   \n",
       "3508   Belgian authorities missed a chance to press a...   \n",
       "7559   Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...   \n",
       "3634   The Latest On Paris Attack: Manhunt Continues;...   \n",
       "8470   18 SHARE The Amish in America have committed t...   \n",
       "6404   We Are Change \\r\\nDonald Trump on Saturday was...   \n",
       "10499  Published on Oct 27, 2016 by Jeff Quitney The ...   \n",
       "9                               and the future Fed rates   \n",
       "10492  Paul Joseph Watson Senior British army officer...   \n",
       "10138  Inside The Mind Of An FBI Informant; Terri Lin...   \n",
       "4953   A couple of weeks ago in this space I pushed b...   \n",
       "496    Nearly 300K New Jobs In February; Unemployment...   \n",
       "5741     WashingtonsBlog \\r\\nBy Robert Parry, the inv...   \n",
       "4131   Jesse Matthew Jr., a former hospital worker, w...   \n",
       "8748     WATCH: Mass Shooting Occurs During #TrumpRio...   \n",
       "6717   By: The Voice of Reason | Regardless of how mu...   \n",
       "2943   Islamic State militants have acknowledged for ...   \n",
       "5248   With only about 70 days left until the electio...   \n",
       "3624   Suspects In Paris Magazine Attack Killed; Mark...   \n",
       "6268    Chart Of The Day: Since 2009 Recovery For The 5%   \n",
       "2738   Before he got to repealing ObamaCare, before h...   \n",
       "4025   The State Department identified two Americans ...   \n",
       "9954   Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                      X1    X2  \n",
       "ID                                                              \n",
       "599                                                 REAL   NaN  \n",
       "10194                                               FAKE   NaN  \n",
       "356                                                 REAL   NaN  \n",
       "2786                                                REAL   NaN  \n",
       "3622                                                REAL   NaN  \n",
       "7375                                                FAKE   NaN  \n",
       "9097                                                FAKE   NaN  \n",
       "9203                                                FAKE   NaN  \n",
       "1602                                                REAL   NaN  \n",
       "4562                                                REAL   NaN  \n",
       "4748                                                REAL   NaN  \n",
       "3508                                                REAL   NaN  \n",
       "7559                                                FAKE   NaN  \n",
       "3634                                                REAL   NaN  \n",
       "8470                                                FAKE   NaN  \n",
       "6404                                                FAKE   NaN  \n",
       "10499                                               FAKE   NaN  \n",
       "9      PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  \n",
       "10492                                               FAKE   NaN  \n",
       "10138                                               FAKE   NaN  \n",
       "4953                                                REAL   NaN  \n",
       "496                                                 REAL   NaN  \n",
       "5741                                                FAKE   NaN  \n",
       "4131                                                REAL   NaN  \n",
       "8748                                                FAKE   NaN  \n",
       "6717                                                FAKE   NaN  \n",
       "2943                                                REAL   NaN  \n",
       "5248                                                REAL   NaN  \n",
       "3624                                                REAL   NaN  \n",
       "6268                            Stagnation for the 95%    FAKE  \n",
       "2738                                                REAL   NaN  \n",
       "4025                                                REAL   NaN  \n",
       "9954                                                FAKE   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displaced_rows = train_data[train_data.X1.notnull()]\n",
    "displaced_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems that the problem lies within the title. For the 31 cases that have only one misplacement (X2 = NaN), the title was splitted into two causing that the actual label is within the X1 column and the article text in the label column. For the double splitted row with index 9, it actually seems that in the title were two commas leading to a double split. However the row with index 6268 repeats the wrongly splitted phrase just again in the label and X1. There is no sign of further text and thus this row should be excluded since it does not provide an actual article text.\n",
    "However, lets start with the rows that have one wrong column break by joining the title and the text field back together into the full title and then replace the text and label column with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data.X1.notnull(), 'title'] = train_data.loc[train_data.X1.notnull(), 'title'] + train_data.loc[train_data.X1.notnull(), 'text']\n",
    "train_data.loc[train_data.X1.notnull(), 'text'] = train_data.loc[train_data.X1.notnull(), 'label']\n",
    "train_data.loc[train_data.X1.notnull(), 'label'] = train_data.loc[train_data.X1.notnull(), 'X1']\n",
    "train_data.loc[train_data.X1.notnull(), 'X1'] = train_data.loc[train_data.X1.notnull(), 'X2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fixing the line break for the first 31 rows, we are now gonna look at the last two displaced rows. While the row with index 9 seems to be easy to fix, the row with id 6268 does not seem to actually have text but only a title. Thus first we are going to fix row 9 and then see if we have more cases in the data set where there is only a title but no text in order to decide how to deal with row 6268."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Planned Parenthood’s lobbying effort pay raise...</td>\n",
       "      <td>and the future Fed rates</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The...</td>\n",
       "      <td>Chart Of The Day: Since 2009 Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "ID                                                        \n",
       "9     Planned Parenthood’s lobbying effort pay raise...   \n",
       "6268  Chart Of The Day: Since 2009—–Recovery For The...   \n",
       "\n",
       "                                                  text  \\\n",
       "ID                                                       \n",
       "9                             and the future Fed rates   \n",
       "6268  Chart Of The Day: Since 2009 Recovery For The 5%   \n",
       "\n",
       "                                                  label    X1    X2  \n",
       "ID                                                                   \n",
       "9     PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  REAL  \n",
       "6268                           Stagnation for the 95%    FAKE  FAKE  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displaced_rows = train_data[train_data.X1.notnull()]\n",
    "displaced_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix row 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[9, 'title'] = train_data.loc[9, 'title'] + \",\" + train_data.loc[9, 'text']\n",
    "train_data.loc[9, 'text'] = train_data.loc[9, 'label']\n",
    "train_data.loc[9, 'label'] = train_data.loc[9, 'X1']\n",
    "train_data.loc[9, 'X1'] = np.nan\n",
    "train_data.loc[9, 'X2'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows with no text\n",
    "Overall we see that we have 21 rows that only have white space within their text column. Furthermore, we see that all these entries are fake, which is also the case for our displaced row 6268. Thus in order to identify these cases more easily we will insert \"zzzzz\" in the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>The Arcturian Group by Marilyn Raffaele Octobe...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>Southern Poverty Law Center Targets Anti-Jihad...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>Refugee Resettlement Watch: Swept Away In Nort...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>Michael Bloomberg Names Technological Unemploy...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>Alert News : Putins Army Is Coming For World W...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>An LDS Reader Takes A Look At Trump Accuser Je...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>America’s Senator Jeff Sessions Warns of Worse...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>Paris Migrant Campers Increase after Calais Is...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>Putins Army is coming for World war 3 against ...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714</th>\n",
       "      <td>Is your promising internet career over now Vin...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>Radio Derb Transcript For October 21 Up: The M...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>A Reader Refers Us To Englishman Pat Condell O...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>“Donald Trump And The Rise Of White Identity I...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>Hope for the best, prepare for the worst…</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>The Comey Confrontation: In Our New Third-Worl...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>NATIONAL REVIEW, Conservatism Inc., Plan To Ca...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>Thomas Frank Explores Whether Hillary Clinton ...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>Democrats Playing Class Card To Split the Whit...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>Comment software has been rolled back to old v...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9070</th>\n",
       "      <td>Round Up the Unusual Suspects: Moneyball Nerds...</td>\n",
       "      <td></td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title text label   X1   X2\n",
       "ID                                                                           \n",
       "5530   The Arcturian Group by Marilyn Raffaele Octobe...       FAKE  NaN  NaN\n",
       "8332   MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...       FAKE  NaN  NaN\n",
       "9314   Southern Poverty Law Center Targets Anti-Jihad...       FAKE  NaN  NaN\n",
       "10304  Refugee Resettlement Watch: Swept Away In Nort...       FAKE  NaN  NaN\n",
       "9474   Michael Bloomberg Names Technological Unemploy...       FAKE  NaN  NaN\n",
       "5802   Alert News : Putins Army Is Coming For World W...       FAKE  NaN  NaN\n",
       "9564   An LDS Reader Takes A Look At Trump Accuser Je...       FAKE  NaN  NaN\n",
       "5752   America’s Senator Jeff Sessions Warns of Worse...       FAKE  NaN  NaN\n",
       "8816   Paris Migrant Campers Increase after Calais Is...       FAKE  NaN  NaN\n",
       "7525   Putins Army is coming for World war 3 against ...       FAKE  NaN  NaN\n",
       "6714   Is your promising internet career over now Vin...       FAKE  NaN  NaN\n",
       "5776   Radio Derb Transcript For October 21 Up: The M...       FAKE  NaN  NaN\n",
       "8055   A Reader Refers Us To Englishman Pat Condell O...       FAKE  NaN  NaN\n",
       "10193  “Donald Trump And The Rise Of White Identity I...       FAKE  NaN  NaN\n",
       "5715           Hope for the best, prepare for the worst…       FAKE  NaN  NaN\n",
       "6733   The Comey Confrontation: In Our New Third-Worl...       FAKE  NaN  NaN\n",
       "5367   NATIONAL REVIEW, Conservatism Inc., Plan To Ca...       FAKE  NaN  NaN\n",
       "5427   Thomas Frank Explores Whether Hillary Clinton ...       FAKE  NaN  NaN\n",
       "8240   Democrats Playing Class Card To Split the Whit...       FAKE  NaN  NaN\n",
       "7048   Comment software has been rolled back to old v...       FAKE  NaN  NaN\n",
       "9070   Round Up the Unusual Suspects: Moneyball Nerds...       FAKE  NaN  NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text = train_data[train_data.text == ' ']\n",
    "empty_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace empty text with zzzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>The Arcturian Group by Marilyn Raffaele Octobe...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>Southern Poverty Law Center Targets Anti-Jihad...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>Refugee Resettlement Watch: Swept Away In Nort...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>Michael Bloomberg Names Technological Unemploy...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>Alert News : Putins Army Is Coming For World W...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>An LDS Reader Takes A Look At Trump Accuser Je...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>America’s Senator Jeff Sessions Warns of Worse...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>Paris Migrant Campers Increase after Calais Is...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>Putins Army is coming for World war 3 against ...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714</th>\n",
       "      <td>Is your promising internet career over now Vin...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>Radio Derb Transcript For October 21 Up: The M...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>A Reader Refers Us To Englishman Pat Condell O...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>“Donald Trump And The Rise Of White Identity I...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>Hope for the best, prepare for the worst…</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>The Comey Confrontation: In Our New Third-Worl...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>NATIONAL REVIEW, Conservatism Inc., Plan To Ca...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>Thomas Frank Explores Whether Hillary Clinton ...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>Democrats Playing Class Card To Split the Whit...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>Comment software has been rolled back to old v...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9070</th>\n",
       "      <td>Round Up the Unusual Suspects: Moneyball Nerds...</td>\n",
       "      <td>zzzzz</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title   text label    X1  \\\n",
       "ID                                                                            \n",
       "5530   The Arcturian Group by Marilyn Raffaele Octobe...  zzzzz  FAKE   NaN   \n",
       "8332   MARKETWATCH LEFTIST: MSM’s “Blatant” Anti Trum...  zzzzz  FAKE   NaN   \n",
       "9314   Southern Poverty Law Center Targets Anti-Jihad...  zzzzz  FAKE   NaN   \n",
       "10304  Refugee Resettlement Watch: Swept Away In Nort...  zzzzz  FAKE   NaN   \n",
       "9474   Michael Bloomberg Names Technological Unemploy...  zzzzz  FAKE   NaN   \n",
       "5802   Alert News : Putins Army Is Coming For World W...  zzzzz  FAKE   NaN   \n",
       "9564   An LDS Reader Takes A Look At Trump Accuser Je...  zzzzz  FAKE   NaN   \n",
       "5752   America’s Senator Jeff Sessions Warns of Worse...  zzzzz  FAKE   NaN   \n",
       "8816   Paris Migrant Campers Increase after Calais Is...  zzzzz  FAKE   NaN   \n",
       "7525   Putins Army is coming for World war 3 against ...  zzzzz  FAKE   NaN   \n",
       "6714   Is your promising internet career over now Vin...  zzzzz  FAKE   NaN   \n",
       "5776   Radio Derb Transcript For October 21 Up: The M...  zzzzz  FAKE   NaN   \n",
       "8055   A Reader Refers Us To Englishman Pat Condell O...  zzzzz  FAKE   NaN   \n",
       "10193  “Donald Trump And The Rise Of White Identity I...  zzzzz  FAKE   NaN   \n",
       "5715           Hope for the best, prepare for the worst…  zzzzz  FAKE   NaN   \n",
       "6733   The Comey Confrontation: In Our New Third-Worl...  zzzzz  FAKE   NaN   \n",
       "5367   NATIONAL REVIEW, Conservatism Inc., Plan To Ca...  zzzzz  FAKE   NaN   \n",
       "5427   Thomas Frank Explores Whether Hillary Clinton ...  zzzzz  FAKE   NaN   \n",
       "8240   Democrats Playing Class Card To Split the Whit...  zzzzz  FAKE   NaN   \n",
       "6268   Chart Of The Day: Since 2009—–Recovery For The...  zzzzz  FAKE  FAKE   \n",
       "7048   Comment software has been rolled back to old v...  zzzzz  FAKE   NaN   \n",
       "9070   Round Up the Unusual Suspects: Moneyball Nerds...  zzzzz  FAKE   NaN   \n",
       "\n",
       "         X2  \n",
       "ID           \n",
       "5530    NaN  \n",
       "8332    NaN  \n",
       "9314    NaN  \n",
       "10304   NaN  \n",
       "9474    NaN  \n",
       "5802    NaN  \n",
       "9564    NaN  \n",
       "5752    NaN  \n",
       "8816    NaN  \n",
       "7525    NaN  \n",
       "6714    NaN  \n",
       "5776    NaN  \n",
       "8055    NaN  \n",
       "10193   NaN  \n",
       "5715    NaN  \n",
       "6733    NaN  \n",
       "5367    NaN  \n",
       "5427    NaN  \n",
       "8240    NaN  \n",
       "6268   FAKE  \n",
       "7048    NaN  \n",
       "9070    NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace text for specific row\n",
    "train_data.loc[6268, 'text'] = \"zzzzz\"\n",
    "train_data.loc[6268, 'label'] = train_data.loc[6268, 'X1']\n",
    "\n",
    "\n",
    "# Fill empty text columns\n",
    "train_data[\"text\"] = train_data.apply(lambda row: row[\"text\"].strip(), axis=1).replace(\"\", \"zzzzz\")\n",
    "\n",
    "# Show zzzzz rows\n",
    "train_data[train_data[\"text\"] == \"zzzzz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3999 entries, 8476 to 9673\n",
      "Data columns (total 5 columns):\n",
      "title    3999 non-null object\n",
      "text     3999 non-null object\n",
      "label    3999 non-null object\n",
      "X1       1 non-null object\n",
      "X2       1 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 347.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fixing the displaced rows we can now delete the columns X1 and X2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \n",
       "ID                                                              \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875    It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[['title', 'text','label']]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set\n",
    "Eventhough the test data set does not show a splitting problem there might be rows that do not have anything in their text column. Thus we are going to check this and then fill the empty columns like we did for the train data set. Apparently there are 15 rows in which the text column is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10414</th>\n",
       "      <td>More on Trump’s Populism and How It Can Be Con...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>Radio Derb transcript for October 29th is up: ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8601</th>\n",
       "      <td>Pro-sovereignty Legislators Demand That Admini...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>World War 3?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>A Mormon Reader Says Most Mormons Will Still B...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254</th>\n",
       "      <td>Paris: Riot Police Flatten Invader Camp</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>Huma Abedin’s Muslim Dad</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9526</th>\n",
       "      <td>Hillary is Sick &amp; Tired of Suffering from Wein...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6800</th>\n",
       "      <td>Automation: Robots from Korea to America Are R...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>WORLD WAR 3 – HILLARY V.S. TRUMP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>A Fifth Clinton Presidency? Hill, No!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>Huma’s Weiner Dogs Hillary</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6127</th>\n",
       "      <td>Radio Derb: Peak White Guilt, PC Now To The LE...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7043</th>\n",
       "      <td>Hillary’s High Crimes &amp; Misdemeanors Threaten ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9337</th>\n",
       "      <td>Radio Derb Is On The Air–Leonardo And Brazil’s...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title text\n",
       "ID                                                           \n",
       "10414  More on Trump’s Populism and How It Can Be Con...     \n",
       "6311   Radio Derb transcript for October 29th is up: ...     \n",
       "8601   Pro-sovereignty Legislators Demand That Admini...     \n",
       "8626                                        World War 3?     \n",
       "8548   A Mormon Reader Says Most Mormons Will Still B...     \n",
       "6254             Paris: Riot Police Flatten Invader Camp     \n",
       "8875                            Huma Abedin’s Muslim Dad     \n",
       "9526   Hillary is Sick & Tired of Suffering from Wein...     \n",
       "6800   Automation: Robots from Korea to America Are R...     \n",
       "6773                    WORLD WAR 3 – HILLARY V.S. TRUMP     \n",
       "9467               A Fifth Clinton Presidency? Hill, No!     \n",
       "5324                          Huma’s Weiner Dogs Hillary     \n",
       "6127   Radio Derb: Peak White Guilt, PC Now To The LE...     \n",
       "7043   Hillary’s High Crimes & Misdemeanors Threaten ...     \n",
       "9337   Radio Derb Is On The Air–Leonardo And Brazil’s...     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_text_test = test_data[test_data.text == ' ']\n",
    "empty_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"text\"] = test_data.apply(lambda row: row[\"text\"].strip(), axis=1).replace(\"\", \"zzzzz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "Since we are working on a classification problem, it is important to look at the target distribution. Highly imbalanced targets need resampling methods in order to train a well-working machine learning model. Thus our first step in terms of data exploration is to check the amount of fake and real labels within our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    2008\n",
       "text     2008\n",
       "label    2008\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.label == \"REAL\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    1991\n",
       "text     1991\n",
       "label    1991\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.label == \"FAKE\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is almost equally distributed with 2008 real and 1990 fake articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real news vs. fake news\n",
    "\n",
    "<font color=red>\n",
    "   \n",
    "- Common words text\n",
    "- Common words title\n",
    "\n",
    "- Common words text (w/o stopwords, lowercase)\n",
    "- Common words title (w/o stopwords, lowercase)\n",
    "\n",
    "- Distribution of word classes overall (Adj, noun, ...)\n",
    "\n",
    "- length of title\n",
    "- length of text\n",
    "\n",
    "- lexical diversity\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preparation\n",
    "\n",
    "In order to work with the title and text, the columns need to be cleaned. We follow several steps in order to prepare our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 POS tagging\n",
    "The first step before we go into normalizing our texts, is part-of-speech (POS) tagging. POS tagging determines which role a word plays within a sentence. The four most common tags are noun, verb, adjective and adverb. \n",
    "\n",
    "We are using POS tagging in order to tag all the words within our title and text to use this as input for the following lemmatization. An easy approach is to apply lemmatization tagging all words either as a verb or a noun. However, this will not always result in a very desirable lemmatization output. Thus we created two function that use the nltk pos_tag function to pos tag the words (postag_text) and then based on these tags we assigned these tags to the four main tag categories (get_wordnet_pos) which are then used beside the word itself as input for our lemmatization process. \n",
    "\n",
    "Within the postag_text function it can be determined to remove stopwords, lowercase the words and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #by default is noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postag_text(txt, rm_stopwords=True, lowertext=True, rm_punct=True):\n",
    "    if rm_punct:\n",
    "        txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "        \n",
    "    tokenized = word_tokenize(txt)\n",
    "    if lowertext:\n",
    "        tokenized = list(map(lambda word: word.lower(), tokenized))\n",
    "        \n",
    "    if rm_stopwords:\n",
    "        sw = set(stopwords.words('english'))\n",
    "        tokenized = list(filter(lambda word: word not in sw, tokenized))\n",
    "        \n",
    "    tags = nltk.pos_tag(tokenized)\n",
    "    return list(map(lambda tag_info: (tag_info[0], get_wordnet_pos(tag_info[1])), tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lemmatizing\n",
    "\n",
    "Lemmatizing is trying to find the root of a word and replaces this word with its root. For verbs this is the infinitive thus are/is/was etc. is turned to \"be\". Four nouns the root is usually the singular form. However, there are also other noun transformation such as working, which has the root work. Lemmatizing transforms mainly nouns and in particular verbs, however also adjective can be affected for example by using comparatives and superlatives.\n",
    "\n",
    "We used the nltk WordNetLemmatizer for lemmatizing our texts. The function we created first tags all the words, which are already lowercased. We decided not to exclude stopwords in this particular step since we are going to use the clean text also to create n-grams in which we think stopwords can actually be helpful. For the same reason punctuation is not removed at this point since for n-grams this is needed due to the fact that n-grams should not go over the end of a sentence. However when we use usual unigrams for vectorization stopwords and punctuation (actually everthing that is not a letter) will be removed. Luckily the WordNetLemmatizer is actually able to lemmatize a word at the end of a sentence eventhough it is connected to a period, question mark etc. (see below).\n",
    "\n",
    "The next step in this function is to actually lemmatize the words and here as said before we did not include a default tag but used the actually assigned tag from our POS tagging process. At the end the function returns the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, lowertext=True, rm_punct=False):\n",
    "    wnlt = WordNetLemmatizer()\n",
    "    \n",
    "    tagged = postag_text(text, rm_stopwords=False, rm_punct=rm_punct)\n",
    "    \n",
    "    lemmatized_list = list(map(lambda tag: wnlt.lemmatize(tag[0], pos=tag[1]), tagged))\n",
    "    \n",
    "    lemmatized_text = reduce(lambda x, y: x + \" \" + y, lemmatized_list)\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog run , he run , she be drink , people be sit\n",
      "dog run , he run , she be drink , people be sit ?\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting\"))\n",
    "print(lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming\n",
    "The next step after lemmatizing is stemming. Stemming is a much more \"brutal\" transformation than lemmatizing since stemming actually cuts off words. Most of the time from what we see stemming is applied after lemmatization if the POS tag 'verb' was used for all words in the text. For example, if lemmatization is applied and the default is verb all plural nouns are not transformed to their singular root. However, if stemming is afterwards applied the stemmer will take care of this problem. Since we use individual POS tags for each word we are not facing this problem. However, in order to reduce the overall lexical diversity of our text and to be able to identify words with the actual same meaning we also apply stemming in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Why are we using word_tokenize here and not our pos tags? I get that this is on the lemmatized stuff, but why here word_tokenizer?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, lowertext=True, rm_punct=False):\n",
    "    pst = PorterStemmer()\n",
    "    \n",
    "    if rm_punct:\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    \n",
    "    if lowertext:\n",
    "        tokenized = list(map(lambda word: word.lower(), tokenized))\n",
    "    \n",
    "    stemmed = reduce(lambda x, y: x + \" \" + y, list(map(lambda word: pst.stem(word), tokenized)))\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create cleaned columns\n",
    "Now we are going to apply first lemmatizing (incl. POS tagging) and then stemming to our title and text columns. In order to do so we created a new function that combines those two steps. As seen in the example, while stemming only cuts of the word endings, including lemmatizing actually helps to find the root of words such as be or child.\n",
    "\n",
    "For our dataframe we are now creating not only the cleaned title and text column by using lemmatizing and stemming but also combine the two cleaned columns into a third column that merges title and text.\n",
    "\n",
    "Within the clean columns we still have stop words and punctuation but all the words are already lowercased. Title and text are also separated by a period. As said before we are doing this for the later creation of n-grams and due to the fact that the vectorizers actually are able to remove stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lemmatize_text(text, lowertext=True, rm_punct=False):\n",
    "    lemmatized = lemmatize_text(text, lowertext=lowertext, rm_punct=rm_punct)\n",
    "    return stem_text(lemmatized, lowertext=lowertext, rm_punct=rm_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col(df, col):\n",
    "    return df.apply(lambda row: stem_lemmatize_text(row[col]), axis=1).to_frame(name=\"clean_{}\".format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog run , he ran , she is drink , peopl are sit , children were game\n",
      "dog run , he run , she be drink , people be sit , child be game\n",
      "dog run , he run , she be drink , peopl be sit , child be game\n"
     ]
    }
   ],
   "source": [
    "print(stem_text(\"Dogs run, he ran, she is drinking, people are sitting, children were gaming\"))\n",
    "print(lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting, children were gaming\"))\n",
    "print(stem_lemmatize_text(\"Dogs run, he ran, she is drinking, people are sitting, children were gaming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillari ’ s fear</td>\n",
       "      <td>daniel greenfield , a shillman journal fellow ...</td>\n",
       "      <td>you can smell hillari ’ s fear. daniel greenfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi</td>\n",
       "      <td>u.s. secretari of state john f. kerri say mond...</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi. u.s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>— kayde king ( @ kaydeek ) novemb 9 , 2016 the...</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battl of new york : whi thi primari matter</td>\n",
       "      <td>it 's primari day in new york and front-runn h...</td>\n",
       "      <td>the battl of new york : whi thi primari matter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0                     you can smell hillari ’ s fear   \n",
       "1  watch the exact moment paul ryan commit polit ...   \n",
       "2          kerri to go to pari in gestur of sympathi   \n",
       "3  berni support on twitter erupt in anger agains...   \n",
       "4     the battl of new york : whi thi primari matter   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  daniel greenfield , a shillman journal fellow ...   \n",
       "1  googl pinterest digg linkedin reddit stumbleup...   \n",
       "2  u.s. secretari of state john f. kerri say mond...   \n",
       "3  — kayde king ( @ kaydeek ) novemb 9 , 2016 the...   \n",
       "4  it 's primari day in new york and front-runn h...   \n",
       "\n",
       "                                      clean_combined  \n",
       "0  you can smell hillari ’ s fear. daniel greenfi...  \n",
       "1  watch the exact moment paul ryan commit polit ...  \n",
       "2  kerri to go to pari in gestur of sympathi. u.s...  \n",
       "3  berni support on twitter erupt in anger agains...  \n",
       "4  the battl of new york : whi thi primari matter...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    train_data_clean = pd.read_csv(\"data/train_data_clean.csv\")\n",
    "except:\n",
    "    print(\"File not found. Generating clean text...\")\n",
    "    clean_text = clean_col(train_data, \"text\")\n",
    "    clean_title = clean_col(train_data, \"title\")\n",
    "    clean_combined = clean_title[\"clean_title\"] + \". \" + clean_text[\"clean_text\"]\n",
    "    clean_combined = clean_combined.rename(\"clean_combined\")\n",
    "\n",
    "    train_data_clean = pd.concat([train_data,\n",
    "                            clean_title,\n",
    "                            clean_text,\n",
    "                            clean_combined\n",
    "                           ], axis=1)\n",
    "\n",
    "    train_data_clean.to_csv(\"data/train_data_clean.csv\")\n",
    "\n",
    "train_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 level !</td>\n",
       "      <td>septemb new home sale rise back to 1992 level ...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 leve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>but when congress debat and pass the patient p...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>the berni sander and ted cruz campaign vow to ...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>polic search for the second of two escap priso...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>no matter who win california 's 475 deleg on t...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...   \n",
       "1  But when Congress debated and passed the Patie...   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "3  Police searching for the second of two escaped...   \n",
       "4  No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  septemb new home sale rise——-back to 1992 level !   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...   \n",
       "2  sander , cruz resist pressur after ny loss , v...   \n",
       "3  surviv escap prison like fatigu and prone to m...   \n",
       "4  clinton and sander neck and neck in california...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  septemb new home sale rise back to 1992 level ...   \n",
       "1  but when congress debat and pass the patient p...   \n",
       "2  the berni sander and ted cruz campaign vow to ...   \n",
       "3  polic search for the second of two escap priso...   \n",
       "4  no matter who win california 's 475 deleg on t...   \n",
       "\n",
       "                                      clean_combined  \n",
       "0  septemb new home sale rise——-back to 1992 leve...  \n",
       "1  whi the obamacar doomsday cult ca n't admit it...  \n",
       "2  sander , cruz resist pressur after ny loss , v...  \n",
       "3  surviv escap prison like fatigu and prone to m...  \n",
       "4  clinton and sander neck and neck in california...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    test_data_clean = pd.read_csv(\"data/test_data_clean.csv\")\n",
    "except:\n",
    "    print(\"File not found. Generating clean text...\")\n",
    "    clean_text = clean_col(test_data, \"text\")\n",
    "    clean_title = clean_col(test_data, \"title\")\n",
    "    clean_combined = clean_title[\"clean_title\"] + \". \" + clean_text[\"clean_text\"]\n",
    "    clean_combined = clean_combined.rename(\"clean_combined\")\n",
    "\n",
    "    test_data_clean = pd.concat([test_data,\n",
    "                            clean_title,\n",
    "                            clean_text,\n",
    "                            clean_combined\n",
    "                           ], axis=1)\n",
    "\n",
    "    test_data_clean.to_csv(\"data/test_data_clean.csv\")\n",
    "\n",
    "test_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data set we are going to create several features based on our columns. We are not going to include all features right aways or all at the same time but since we are already working with the data it makes sense to start creating features that will later be included and tried within the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 POS-tag distribution\n",
    "\n",
    "The first feature that we created is related to the POS tags within the title and the text. As seen before in our data preparation part we created a function to tag the title and the text using four different tags. For each title as well as each text we are now using these tags and calculate the actual distribution of nouns, verbs, adjectives and adverbs within each title and each text. These number range from 0 to 1 reflecting the percentage of the title or text that are reflected by one of the four tags. We are not going create these features for the combined column consisting of title and text since most of the time the distribution will be close to the one from the text column. Overall we end up with eight new columns (=features), four of them for title and four of them for text. These four columns types represent the respecitve percentage of one of the four tags within the title or text.\n",
    "\n",
    "In order to create this distribution the text was not only lowercased but also only letter were included as well as stop words were removed. We took these steps in order to only include words that hold actual informative value and calculate the distribution based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_counts(pos_tags):\n",
    "    counts = {}\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in counts:\n",
    "            counts[tag] += 1\n",
    "        else:\n",
    "            counts[tag] = 1\n",
    "    return counts\n",
    "\n",
    "def tag_dists(counts):\n",
    "    total_words = sum(counts.values())\n",
    "    dists = {}\n",
    "    for tag, count in counts.items():\n",
    "        dists[tag] = count / total_words\n",
    "    return dists\n",
    "\n",
    "def create_dists_series(text):\n",
    "    return tag_dists(tag_counts(postag_text(text)))\n",
    "\n",
    "def create_dists_df(df, col):\n",
    "    return pd.DataFrame(list(df.apply(lambda row: create_dists_series(row[col]), axis=1)))\n",
    "    #     as_series = df.apply(lambda row: create_dists_row(row[\"text\"]), axis=1)\n",
    "\n",
    "    #     pd.Dataframe({\"n\": as_series[\"n\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title_dists(df):\n",
    "    df = create_dists_df(df, \"title\")\n",
    "\n",
    "    df = df.rename(columns={'a': 'title_adj_dist',\n",
    "                            'n': 'title_noun_dist',\n",
    "                            'v': 'title_verb_dist',\n",
    "                            'r': 'title_adverb_dist'})\n",
    "    # Titles are short, certain word types may not appear\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_text_dists(df):\n",
    "    df = create_dists_df(df, \"text\")\n",
    "\n",
    "    df = df.rename(columns={'a': 'text_adj_dist',\n",
    "                            'n': 'text_noun_dist',\n",
    "                            'v': 'text_verb_dist',\n",
    "                            'r': 'text_adverb_dist'})\n",
    "    # There are some articles with basically no content (e.g. just a date for a picture)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillari ’ s fear</td>\n",
       "      <td>daniel greenfield , a shillman journal fellow ...</td>\n",
       "      <td>you can smell hillari ’ s fear. daniel greenfi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210448</td>\n",
       "      <td>0.505970</td>\n",
       "      <td>0.073134</td>\n",
       "      <td>0.210448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.545833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.220833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi</td>\n",
       "      <td>u.s. secretari of state john f. kerri say mond...</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi. u.s...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.514523</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.207469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>— kayde king ( @ kaydeek ) novemb 9 , 2016 the...</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>0.497942</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.234568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battl of new york : whi thi primari matter</td>\n",
       "      <td>it 's primari day in new york and front-runn h...</td>\n",
       "      <td>the battl of new york : whi thi primari matter...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196629</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.247191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0   8476                       You Can Smell Hillary’s Fear   \n",
       "1      1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2      2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3      3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4      4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0                     you can smell hillari ’ s fear   \n",
       "1  watch the exact moment paul ryan commit polit ...   \n",
       "2          kerri to go to pari in gestur of sympathi   \n",
       "3  berni support on twitter erupt in anger agains...   \n",
       "4     the battl of new york : whi thi primari matter   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  daniel greenfield , a shillman journal fellow ...   \n",
       "1  googl pinterest digg linkedin reddit stumbleup...   \n",
       "2  u.s. secretari of state john f. kerri say mond...   \n",
       "3  — kayde king ( @ kaydeek ) novemb 9 , 2016 the...   \n",
       "4  it 's primari day in new york and front-runn h...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  you can smell hillari ’ s fear. daniel greenfi...        0.333333   \n",
       "1  watch the exact moment paul ryan commit polit ...        0.181818   \n",
       "2  kerri to go to pari in gestur of sympathi. u.s...        0.200000   \n",
       "3  berni support on twitter erupt in anger agains...        0.125000   \n",
       "4  the battl of new york : whi thi primari matter...        0.400000   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.666667                0.0         0.000000       0.210448   \n",
       "1         0.727273                0.0         0.090909       0.150000   \n",
       "2         0.600000                0.0         0.200000       0.253112   \n",
       "3         0.625000                0.0         0.250000       0.226337   \n",
       "4         0.600000                0.0         0.000000       0.196629   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  \n",
       "0        0.505970          0.073134        0.210448  \n",
       "1        0.545833          0.083333        0.220833  \n",
       "2        0.514523          0.024896        0.207469  \n",
       "3        0.497942          0.041152        0.234568  \n",
       "4        0.528090          0.028090        0.247191  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean_dist = pd.concat([train_data_clean.reset_index(),\n",
    "                                   create_title_dists(train_data_clean), \n",
    "                                   create_text_dists(train_data_clean)], \n",
    "                                  axis=1)\n",
    "train_data_clean_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 level !</td>\n",
       "      <td>septemb new home sale rise back to 1992 level ...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 leve...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.196429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>but when congress debat and pass the patient p...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189369</td>\n",
       "      <td>0.508306</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.225914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>the berni sander and ted cruz campaign vow to ...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.542923</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.245940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>polic search for the second of two escap priso...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.143087</td>\n",
       "      <td>0.540193</td>\n",
       "      <td>0.059486</td>\n",
       "      <td>0.257235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>no matter who win california 's 475 deleg on t...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.187898</td>\n",
       "      <td>0.506369</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.238854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1      1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2      2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3      3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4      4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...   \n",
       "1  But when Congress debated and passed the Patie...   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "3  Police searching for the second of two escaped...   \n",
       "4  No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  septemb new home sale rise——-back to 1992 level !   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...   \n",
       "2  sander , cruz resist pressur after ny loss , v...   \n",
       "3  surviv escap prison like fatigu and prone to m...   \n",
       "4  clinton and sander neck and neck in california...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  septemb new home sale rise back to 1992 level ...   \n",
       "1  but when congress debat and pass the patient p...   \n",
       "2  the berni sander and ted cruz campaign vow to ...   \n",
       "3  polic search for the second of two escap priso...   \n",
       "4  no matter who win california 's 475 deleg on t...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  septemb new home sale rise——-back to 1992 leve...        0.142857   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...        0.200000   \n",
       "2  sander , cruz resist pressur after ny loss , v...        0.222222   \n",
       "3  surviv escap prison like fatigu and prone to m...        0.222222   \n",
       "4  clinton and sander neck and neck in california...        0.166667   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.428571           0.142857         0.285714       0.214286   \n",
       "1         0.800000           0.000000         0.000000       0.189369   \n",
       "2         0.555556           0.000000         0.222222       0.155452   \n",
       "3         0.333333           0.000000         0.444444       0.143087   \n",
       "4         0.333333           0.166667         0.333333       0.187898   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  \n",
       "0        0.517857          0.071429        0.196429  \n",
       "1        0.508306          0.076412        0.225914  \n",
       "2        0.542923          0.055684        0.245940  \n",
       "3        0.540193          0.059486        0.257235  \n",
       "4        0.506369          0.066879        0.238854  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_clean_dist = pd.concat([test_data_clean.reset_index(),\n",
    "                                   create_title_dists(test_data_clean), \n",
    "                                   create_text_dists(test_data_clean)], \n",
    "                                  axis=1)\n",
    "test_data_clean_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lexical diversity\n",
    "\n",
    "The next feature is the lexical diversity, which reflects the ratio of different unique words/words stems to the total number of words within a text. The score is between 0 and 1 and the higher the lexcial diversity the more unique different words are included within a text. Lexical diversity belongs to the concept of lexical richness, for example while 'animal', 'beast', 'creature' can all discribe the same concept, these words are not used in the same way or concept. \n",
    "\n",
    "\n",
    "<font color=red>\n",
    "\n",
    "One of our thoughts is that real news should be more linked to fact based reporting while fake news rather use catchy phrases, the lexical diversity score for real news might tend to be higher compared to fake news. Even some rethorical techniques make use of repeating a word several time throughout a speech. Our suspision would be that those techniques are more used in fake than in real news articles. \n",
    "\n",
    "\n",
    "WE NEED TO PLOT THIS!\n",
    "</font>\n",
    "\n",
    "In order to create the lexical diversity score we used the clean_combined column that stores lemmatized and stemmed text and title. Furthermore, we only used letters (removing special characters and numbers) and excluded stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    # remove everything except letter (including numbers)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    sw = set(stopwords.words('english'))\n",
    "    tokenized = list(filter(lambda word: word not in sw, tokenized))\n",
    "    \n",
    "    try:\n",
    "        ld = len(set(tokenized)) / len(tokenized)\n",
    "    except ZeroDivisionError as e: # Can happen if the entire body is numbers/date\n",
    "        ld = 1.0\n",
    "    \n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div_col(df):\n",
    "    return df.apply(lambda row: lexical_diversity(row[\"clean_combined\"]), axis=1).rename(\"lexical_diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "      <th>lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillari ’ s fear</td>\n",
       "      <td>daniel greenfield , a shillman journal fellow ...</td>\n",
       "      <td>you can smell hillari ’ s fear. daniel greenfi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210448</td>\n",
       "      <td>0.505970</td>\n",
       "      <td>0.073134</td>\n",
       "      <td>0.210448</td>\n",
       "      <td>0.592754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n",
       "      <td>watch the exact moment paul ryan commit polit ...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.545833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.220833</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi</td>\n",
       "      <td>u.s. secretari of state john f. kerri say mond...</td>\n",
       "      <td>kerri to go to pari in gestur of sympathi. u.s...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.514523</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.207469</td>\n",
       "      <td>0.678431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>— kayde king ( @ kaydeek ) novemb 9 , 2016 the...</td>\n",
       "      <td>berni support on twitter erupt in anger agains...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>0.497942</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.699620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battl of new york : whi thi primari matter</td>\n",
       "      <td>it 's primari day in new york and front-runn h...</td>\n",
       "      <td>the battl of new york : whi thi primari matter...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196629</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0   8476                       You Can Smell Hillary’s Fear   \n",
       "1      1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2      2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3      3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4      4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0                     you can smell hillari ’ s fear   \n",
       "1  watch the exact moment paul ryan commit polit ...   \n",
       "2          kerri to go to pari in gestur of sympathi   \n",
       "3  berni support on twitter erupt in anger agains...   \n",
       "4     the battl of new york : whi thi primari matter   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  daniel greenfield , a shillman journal fellow ...   \n",
       "1  googl pinterest digg linkedin reddit stumbleup...   \n",
       "2  u.s. secretari of state john f. kerri say mond...   \n",
       "3  — kayde king ( @ kaydeek ) novemb 9 , 2016 the...   \n",
       "4  it 's primari day in new york and front-runn h...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  you can smell hillari ’ s fear. daniel greenfi...        0.333333   \n",
       "1  watch the exact moment paul ryan commit polit ...        0.181818   \n",
       "2  kerri to go to pari in gestur of sympathi. u.s...        0.200000   \n",
       "3  berni support on twitter erupt in anger agains...        0.125000   \n",
       "4  the battl of new york : whi thi primari matter...        0.400000   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.666667                0.0         0.000000       0.210448   \n",
       "1         0.727273                0.0         0.090909       0.150000   \n",
       "2         0.600000                0.0         0.200000       0.253112   \n",
       "3         0.625000                0.0         0.250000       0.226337   \n",
       "4         0.600000                0.0         0.000000       0.196629   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  lexical_diversity  \n",
       "0        0.505970          0.073134        0.210448           0.592754  \n",
       "1        0.545833          0.083333        0.220833           0.763158  \n",
       "2        0.514523          0.024896        0.207469           0.678431  \n",
       "3        0.497942          0.041152        0.234568           0.699620  \n",
       "4        0.528090          0.028090        0.247191           0.625000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_features = pd.concat([train_data_clean_dist, lex_div_col(train_data_clean_dist)], axis=1)\n",
    "train_all_features.to_csv(\"data/train_all_features.csv\")\n",
    "train_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_combined</th>\n",
       "      <th>title_adj_dist</th>\n",
       "      <th>title_noun_dist</th>\n",
       "      <th>title_adverb_dist</th>\n",
       "      <th>title_verb_dist</th>\n",
       "      <th>text_adj_dist</th>\n",
       "      <th>text_noun_dist</th>\n",
       "      <th>text_adverb_dist</th>\n",
       "      <th>text_verb_dist</th>\n",
       "      <th>lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 level !</td>\n",
       "      <td>septemb new home sale rise back to 1992 level ...</td>\n",
       "      <td>septemb new home sale rise——-back to 1992 leve...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.784615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>but when congress debat and pass the patient p...</td>\n",
       "      <td>whi the obamacar doomsday cult ca n't admit it...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189369</td>\n",
       "      <td>0.508306</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.225914</td>\n",
       "      <td>0.701258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>the berni sander and ted cruz campaign vow to ...</td>\n",
       "      <td>sander , cruz resist pressur after ny loss , v...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.542923</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.245940</td>\n",
       "      <td>0.582781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>polic search for the second of two escap priso...</td>\n",
       "      <td>surviv escap prison like fatigu and prone to m...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.143087</td>\n",
       "      <td>0.540193</td>\n",
       "      <td>0.059486</td>\n",
       "      <td>0.257235</td>\n",
       "      <td>0.570552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>no matter who win california 's 475 deleg on t...</td>\n",
       "      <td>clinton and sander neck and neck in california...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.187898</td>\n",
       "      <td>0.506369</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.238854</td>\n",
       "      <td>0.563636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     ID                                              title  \\\n",
       "0      0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1      1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2      2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3      3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4      4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...   \n",
       "1  But when Congress debated and passed the Patie...   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "3  Police searching for the second of two escaped...   \n",
       "4  No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  septemb new home sale rise——-back to 1992 level !   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...   \n",
       "2  sander , cruz resist pressur after ny loss , v...   \n",
       "3  surviv escap prison like fatigu and prone to m...   \n",
       "4  clinton and sander neck and neck in california...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  septemb new home sale rise back to 1992 level ...   \n",
       "1  but when congress debat and pass the patient p...   \n",
       "2  the berni sander and ted cruz campaign vow to ...   \n",
       "3  polic search for the second of two escap priso...   \n",
       "4  no matter who win california 's 475 deleg on t...   \n",
       "\n",
       "                                      clean_combined  title_adj_dist  \\\n",
       "0  septemb new home sale rise——-back to 1992 leve...        0.142857   \n",
       "1  whi the obamacar doomsday cult ca n't admit it...        0.200000   \n",
       "2  sander , cruz resist pressur after ny loss , v...        0.222222   \n",
       "3  surviv escap prison like fatigu and prone to m...        0.222222   \n",
       "4  clinton and sander neck and neck in california...        0.166667   \n",
       "\n",
       "   title_noun_dist  title_adverb_dist  title_verb_dist  text_adj_dist  \\\n",
       "0         0.428571           0.142857         0.285714       0.214286   \n",
       "1         0.800000           0.000000         0.000000       0.189369   \n",
       "2         0.555556           0.000000         0.222222       0.155452   \n",
       "3         0.333333           0.000000         0.444444       0.143087   \n",
       "4         0.333333           0.166667         0.333333       0.187898   \n",
       "\n",
       "   text_noun_dist  text_adverb_dist  text_verb_dist  lexical_diversity  \n",
       "0        0.517857          0.071429        0.196429           0.784615  \n",
       "1        0.508306          0.076412        0.225914           0.701258  \n",
       "2        0.542923          0.055684        0.245940           0.582781  \n",
       "3        0.540193          0.059486        0.257235           0.570552  \n",
       "4        0.506369          0.066879        0.238854           0.563636  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_features = pd.concat([test_data_clean_dist, lex_div_col(test_data_clean_dist)], axis=1)\n",
    "test_all_features.to_csv(\"data/test_all_features.csv\")\n",
    "test_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing our train and test data sets we are going to create the function that later vectorize our texts. For machine learning models in NLP we actually transform our texts into vectors. These vectors show how often a word / n-gram (=column) appears in a text (=row). To create our vectors we use the CountVectorizer that already removes punctuation and has an option to remove stopwords. When we are creating unigrams we are removing stop words, while for ngrams we are keeping stopwords. The CountVectorizer is also used to create vectors for bigrams and trigrams. This can be done just by defining the ngram range within CountVectorizer: bigram = ngram_range=[2,2], trigram = ngram_range=[3,3].\n",
    "\n",
    "At the beginning of our modelling we will start with unigrams and later in the modelling part compare it to the results of using bi-grams and tri-grams.\n",
    "\n",
    "For these different types of vectors we created functions that take in a specific column of a data frame that is supposed to be vectorized. As default we set our clean_combined column, however these functions can also be applied to other columns such as the clean_title or clean_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names()).drop(\"label\", axis=1) # hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(ngram_range=[2,2])\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "# def trigram_counts(df, col=\"clean_combined\"):\n",
    "#     vec = CountVectorizer(ngram_range=[3,3])\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "DESCRIBE TF_IDF\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_idf(df, col=\"clean_combined\"):\n",
    "#     vec = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "#     X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "#     return pd.DataFrame(X.toarray(), columns=vec.get_feature_names()).drop(\"label\", axis=1) # hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification modelling\n",
    "In order to evaluate the performance of our models we will use 5-folds cross-validation.\n",
    "\n",
    "<font color=red>\n",
    "\n",
    "- Naive Bayes\n",
    "- Max Entropy Classifier (Logistic Regression)\n",
    "- SVM Classifier\n",
    "\n",
    "Furhter steps\n",
    "- Include bi-grams, tri-grams\n",
    "- Include TF-IDF\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've had a bit of issues with memory errors, so clean our workspace of everything we don't need at the moment\n",
    "\n",
    "del train_data\n",
    "del train_data_clean\n",
    "del test_data\n",
    "del test_data_clean\n",
    "del test_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(data, model_meta, folds=5, drop=[]):\n",
    "    \"\"\"\n",
    "    Perform Grid Search Cross Validation on the input model\n",
    "\n",
    "    :param data: a pandas dataframe where each row is an hour\n",
    "    :param model_meta: An dict containing the name for the model (\"name\"), the sklearn estimator (\"model\"),\n",
    "                    and the parameters for Grid Search Cross Validation (\"params\")\n",
    "    :param folds: The number of splits for cross validation\n",
    "    :return: a tuple containing the best accuracy score found, the parameters used to obtain that score,\n",
    "                and the estimator retrained on the whole dataset\n",
    "    \"\"\"\n",
    "    model = model_meta[\"model\"]\n",
    "    model_params = model_meta[\"params\"]\n",
    "    model_name = model_meta[\"name\"]\n",
    "\n",
    "    to_drop = drop+[\"label\", \"index\", \"ID\", \"text\", \"title\", \"clean_text\", \"clean_title\", \"clean_combined\"]\n",
    "    X = data.drop(columns=to_drop)\n",
    "    y = data[\"label\"]\n",
    "\n",
    "    kfold = KFold(n_splits=folds)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_params, scoring=\"accuracy\", cv=kfold, refit=True)\n",
    "    \n",
    "    #print(\"Shape: {}\".format(y.iloc[0:1,]))\n",
    "    #print(X.head())\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"\\tAverage result for best {}: {} +/- {:.5f}\"\n",
    "          .format(model_name,\n",
    "                  grid_search.best_score_,\n",
    "                  grid_search.cv_results_[\"std_test_score\"][np.argmax(grid_search.cv_results_[\"mean_test_score\"])]))\n",
    "\n",
    "    print(\"\\tBest parameters for {0}: {1}\".format(model_name, grid_search.best_params_))\n",
    "\n",
    "    # Need metrics to choose model, best estimator will have already been retrained on whole data set\n",
    "    \n",
    "    retval = ({\n",
    "        \"model_name\": [model_name],\n",
    "        \"best_acc\": [grid_search.best_score_],\n",
    "        \"best_params\": [str(grid_search.best_params_)]\n",
    "    }, grid_search.best_estimator_)\n",
    "    \n",
    "    return retval    \n",
    "    \n",
    "    #return model_name, grid_search.best_score_, grid_search.best_params_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(result):\n",
    "    meta_info = result[0]\n",
    "    model = result[1]\n",
    "    \n",
    "    new_row = pd.DataFrame(meta_info)\n",
    "    model_results = pd.read_csv(\"models/model_results.csv\")\n",
    "    new_results = pd.concat([model_results, new_row], ignore_index=True)  #model_result.append(new_row)\n",
    "    new_results.to_csv(\"models/model_results.csv\", index=False)\n",
    "    \n",
    "    dump(model, \"models/{}.joblib\".format(meta_info[\"model_name\"][0]))\n",
    "         \n",
    "    return new_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAverage result for best logit_no_vectorization: 0.5743935983995999 +/- 0.00883\n",
      "\tBest parameters for logit_no_vectorization: {'penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc        best_params\n",
       "0  logit_no_vectorization  0.574394  {'penalty': 'l2'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_no_vectorization = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_no_vectorization\",\n",
    "    \"params\": {\n",
    "        \"penalty\": [\"l1\", \"l2\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "save_result(grid_search_model(train_all_features, logit_no_vectorization))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrammed Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_grid_search(data, model_meta, folds=5, col=\"clean_combined\"):\n",
    "    model = model_meta[\"model\"]\n",
    "    model_params = model_meta[\"params\"]\n",
    "    model_name = model_meta[\"name\"]\n",
    "    vectorizer = model_meta[\"vectorizer\"]\n",
    "    \n",
    "    pipe = make_pipeline(vectorizer, model)\n",
    "\n",
    "    X = data[col]\n",
    "    y = data[\"label\"]\n",
    "\n",
    "    kfold = KFold(n_splits=folds)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=model_params, scoring=\"accuracy\", cv=kfold, refit=True)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"\\tAverage result for best {}: {} +/- {:.5f}\"\n",
    "          .format(model_name,\n",
    "                  grid_search.best_score_,\n",
    "                  grid_search.cv_results_[\"std_test_score\"][np.argmax(grid_search.cv_results_[\"mean_test_score\"])]))\n",
    "\n",
    "    print(\"\\tBest parameters for {0}: {1}\".format(model_name, grid_search.best_params_))\n",
    "\n",
    "    # Need metrics to choose model, best estimator will have already been retrained on whole data set\n",
    "    \n",
    "    retval = ({\n",
    "        \"model_name\": [model_name],\n",
    "        \"best_acc\": [grid_search.best_score_],\n",
    "        \"best_params\": [str(grid_search.best_params_)]\n",
    "    }, grid_search.best_estimator_)\n",
    "    \n",
    "    return retval    \n",
    "    \n",
    "    #return model_name, grid_search.best_score_, grid_search.best_params_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_title: 0.7991997999499875 +/- 0.01394\n",
      "\tBest parameters for logit_unigrams_title: {'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_title = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_unigrams_title\",\n",
    "    \"params\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[1,1])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              logit_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_unigrams_title: 0.8067016754188547 +/- 0.01153\n",
      "\tBest parameters for nb_unigrams_title: {'multinomialnb__alpha': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unigrams_title = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_unigrams_title\",\n",
    "    \"params\": {'multinomialnb__alpha': [0.5, 1, 2]},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[1,1])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              nb_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams_title: 0.7716929232308077 +/- 0.00985\n",
      "\tBest parameters for svc_unigrams_title: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams_title = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams_title\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=(1,1))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              svc_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_title: 0.7754438609652413 +/- 0.01358\n",
      "\tBest parameters for pa_unigrams_title: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_title = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_unigrams_title\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=(1,1))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              pa_unigrams_title,\n",
    "                             col=\"clean_title\"))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams: 0.90847711927982 +/- 0.00840\n",
      "\tBest parameters for logit_unigrams: {'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_unigrams\",\n",
    "    \"params\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[1,1])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              logit_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_unigrams: 0.8877219304826206 +/- 0.00638\n",
      "\tBest parameters for nb_unigrams: {'multinomialnb__alpha': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unigrams = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_unigrams\",\n",
    "    \"params\": {'multinomialnb__alpha': [0.5, 1, 2]},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[1,1])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              nb_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams: 0.8924731182795699 +/- 0.00878\n",
      "\tBest parameters for svc_unigrams: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7            svc_unigrams  0.892473                                     {}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=(1,1))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              svc_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams: 0.8809702425606402 +/- 0.01259\n",
      "\tBest parameters for pa_unigrams: {'passiveaggressiveclassifier__C': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7            svc_unigrams  0.892473                                     {}\n",
       "8             pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_unigrams\",\n",
    "    \"params\": {\n",
    "        \"passiveaggressiveclassifier__C\": [0, 0.5, 1, 2]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=(1,1))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              pa_unigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_tfidf: 0.9099774943735934 +/- 0.00578\n",
      "\tBest parameters for logit_unigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  best_acc                            best_params\n",
       "0  logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1    logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2       nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3      svc_unigrams_title  0.771693                                     {}\n",
       "4       pa_unigrams_title  0.775444                                     {}\n",
       "5          logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6             nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7            svc_unigrams  0.892473                                     {}\n",
       "8             pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9    logit_unigrams_tfidf  0.909977                                     {}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_tfidf = {\n",
    "    \"model\": LogisticRegression(penalty=\"l2\"),\n",
    "    \"name\": \"logit_unigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              logit_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_unigrams_tfidf: 0.8559639909977494 +/- 0.01136\n",
      "\tBest parameters for nb_unigrams_tfidf: {'multinomialnb__alpha': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  best_acc                            best_params\n",
       "0   logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1     logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2        nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3       svc_unigrams_title  0.771693                                     {}\n",
       "4        pa_unigrams_title  0.775444                                     {}\n",
       "5           logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6              nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7             svc_unigrams  0.892473                                     {}\n",
       "8              pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9     logit_unigrams_tfidf  0.909977                                     {}\n",
       "10       nb_unigrams_tfidf  0.855964          {'multinomialnb__alpha': 0.5}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unigrams_tfidf = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_unigrams_tfidf\",\n",
    "    \"params\": {'multinomialnb__alpha': [0.5, 1, 2]},\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              nb_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams_tfidf: 0.9287321830457614 +/- 0.00670\n",
      "\tBest parameters for svc_unigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  best_acc                            best_params\n",
       "0   logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1     logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2        nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3       svc_unigrams_title  0.771693                                     {}\n",
       "4        pa_unigrams_title  0.775444                                     {}\n",
       "5           logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6              nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7             svc_unigrams  0.892473                                     {}\n",
       "8              pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9     logit_unigrams_tfidf  0.909977                                     {}\n",
       "10       nb_unigrams_tfidf  0.855964          {'multinomialnb__alpha': 0.5}\n",
       "11      svc_unigrams_tfidf  0.928732                                     {}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams_tfidf = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_tfidf: 0.8927231807951987 +/- 0.00863\n",
      "\tBest parameters for pa_unigrams_tfidf: {'passiveaggressiveclassifier__C': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  best_acc                            best_params\n",
       "0   logit_no_vectorization  0.574394                      {'penalty': 'l2'}\n",
       "1     logit_unigrams_title  0.799200  {'logisticregression__penalty': 'l2'}\n",
       "2        nb_unigrams_title  0.806702            {'multinomialnb__alpha': 1}\n",
       "3       svc_unigrams_title  0.771693                                     {}\n",
       "4        pa_unigrams_title  0.775444                                     {}\n",
       "5           logit_unigrams  0.908477  {'logisticregression__penalty': 'l2'}\n",
       "6              nb_unigrams  0.887722          {'multinomialnb__alpha': 0.5}\n",
       "7             svc_unigrams  0.892473                                     {}\n",
       "8              pa_unigrams  0.880970  {'passiveaggressiveclassifier__C': 1}\n",
       "9     logit_unigrams_tfidf  0.909977                                     {}\n",
       "10       nb_unigrams_tfidf  0.855964          {'multinomialnb__alpha': 0.5}\n",
       "11      svc_unigrams_tfidf  0.928732                                     {}\n",
       "12       pa_unigrams_tfidf  0.892723  {'passiveaggressiveclassifier__C': 1}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_unigrams_tfidf\",\n",
    "    \"params\": {\n",
    "        \"passiveaggressiveclassifier__C\": [0, 0.5, 1, 2]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=(1,1))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features,\n",
    "                              pa_unigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF with engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df, col=\"clean_combined\"):\n",
    "    vec = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "    X = vec.fit_transform(list(df[col]))\n",
    "    \n",
    "    return pd.DataFrame(X.toarray(), columns=vec.get_feature_names()).drop(\"label\", axis=1) # hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building unigram tfidf frame...\n"
     ]
    }
   ],
   "source": [
    "## We have had issues with memory, so using our tf_idf which converts the \n",
    "# scipy sparse matrix to a dense numpy array is not going to help. but its the only way i know how\n",
    "# to combine the scipy mtx from the vectorizer and our dataframe of engineered features.\n",
    "# so if we get a huge spike in performance we'll continue, but otherwise we will just move on to. \n",
    "\n",
    "print(\"Building unigram tfidf frame...\")\n",
    "unigram_frame_tfidf = pd.concat([train_all_features,\n",
    "                                 tf_idf(train_all_features, col=\"clean_combined\")],\n",
    "                                axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_tfidf_all_features: 0.9094773693423356 +/- 0.00412\n",
      "\tBest parameters for logit_unigrams_tfidf_all_features: {'penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_unigrams_tfidf_all_features = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_unigrams_tfidf_all_features\",\n",
    "    \"params\": {\n",
    "        \"penalty\": [\"l1\", \"l2\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(grid_search_model(unigram_frame_tfidf,\n",
    "                              logit_unigrams_tfidf_all_features,))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see basically no difference, so let's move on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_bigrams: 0.9102275568892223 +/- 0.00670\n",
      "\tBest parameters for logit_bigrams: {'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_bigrams = {\n",
    "    \"model\": LogisticRegression(),\n",
    "    \"name\": \"logit_bigrams\",\n",
    "    \"params\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\"]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, logit_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best nb_bigrams: 0.8924731182795699 +/- 0.00583\n",
      "\tBest parameters for nb_bigrams: {'multinomialnb__alpha': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_bigrams = {\n",
    "    \"model\": MultinomialNB(),\n",
    "    \"name\": \"nb_bigrams\",\n",
    "    \"params\": {\n",
    "        \"multinomialnb__alpha\": [0, 0.5, 1]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, nb_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_bigrams: 0.8939734933733433 +/- 0.00898\n",
      "\tBest parameters for svc_bigrams: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_bigrams = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_bigrams\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_bigrams: 0.9034758689672419 +/- 0.00632\n",
      "\tBest parameters for pa_bigrams: {'passiveaggressiveclassifier__C': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_bigrams = {\n",
    "    \"model\": PassiveAggressiveClassifier(),\n",
    "    \"name\": \"pa_bigrams\",\n",
    "    \"params\": {\n",
    "        \"passiveaggressiveclassifier__C\": [0, 0.5, 1, 2]\n",
    "    },\n",
    "    \"vectorizer\": CountVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_bigrams))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_bigrams_tfidf: 0.9294823705926482 +/- 0.00883\n",
      "\tBest parameters for svc_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_bigrams_tfidf = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_bigrams_tfidf: 0.929732433108277 +/- 0.01153\n",
      "\tBest parameters for pa_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_bigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=[2,2])\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams and Bigrams TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best logit_unigrams_bigrams_tfidf: 0.8989747436859215 +/- 0.00790\n",
      "\tBest parameters for logit_unigrams_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hopeful that these will yield better results \n",
    "\n",
    "logit_unigrams_bigrams_tfidf = {\n",
    "    \"model\": LogisticRegression(penalty='l2'), # seems to do the best,trying to save tim\n",
    "    \"name\": \"logit_unigrams_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, logit_unigrams_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best svc_unigrams_bigrams_tfidf: 0.9364841210302576 +/- 0.00783\n",
      "\tBest parameters for svc_unigrams_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_unigrams_bigrams_tfidf = {\n",
    "    \"model\": SVC(kernel=\"linear\"),\n",
    "    \"name\": \"svc_unigrams_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, svc_unigrams_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_bigrams_tfidf: 0.9389847461865466 +/- 0.00848\n",
      "\tBest parameters for pa_unigrams_bigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22          pa_unigrams_bigrams_tfidf  0.938985   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_bigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_unigrams_bigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,2))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_unigrams_bigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_trigrams_tfidf: 0.9019754938734683 +/- 0.01288\n",
      "\tBest parameters for pa_trigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pa_trigrams_tfidf</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22          pa_unigrams_bigrams_tfidf  0.938985   \n",
       "23                  pa_trigrams_tfidf  0.901975   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  \n",
       "23                                     {}  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_trigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_trigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(3,3))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_trigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_bigrams_trigrams_tfidf: 0.9234808702175544 +/- 0.00853\n",
      "\tBest parameters for pa_bigrams_trigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pa_trigrams_tfidf</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pa_bigrams_trigrams_tfidf</td>\n",
       "      <td>0.923481</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_name  best_acc  \\\n",
       "0              logit_no_vectorization  0.574394   \n",
       "1                logit_unigrams_title  0.799200   \n",
       "2                   nb_unigrams_title  0.806702   \n",
       "3                  svc_unigrams_title  0.771693   \n",
       "4                   pa_unigrams_title  0.775444   \n",
       "5                      logit_unigrams  0.908477   \n",
       "6                         nb_unigrams  0.887722   \n",
       "7                        svc_unigrams  0.892473   \n",
       "8                         pa_unigrams  0.880970   \n",
       "9                logit_unigrams_tfidf  0.909977   \n",
       "10                  nb_unigrams_tfidf  0.855964   \n",
       "11                 svc_unigrams_tfidf  0.928732   \n",
       "12                  pa_unigrams_tfidf  0.892723   \n",
       "13  logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                      logit_bigrams  0.910228   \n",
       "15                         nb_bigrams  0.892473   \n",
       "16                        svc_bigrams  0.893973   \n",
       "17                         pa_bigrams  0.903476   \n",
       "18                  svc_bigrams_tfidf  0.929482   \n",
       "19                   pa_bigrams_tfidf  0.929732   \n",
       "20       logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21         svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22          pa_unigrams_bigrams_tfidf  0.938985   \n",
       "23                  pa_trigrams_tfidf  0.901975   \n",
       "24          pa_bigrams_trigrams_tfidf  0.923481   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  \n",
       "23                                     {}  \n",
       "24                                     {}  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_bigrams_trigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_bigrams_trigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(2,3))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_bigrams_trigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search and saving result....\n",
      "\tAverage result for best pa_unigrams_bigrams_trigrams_tfidf: 0.9314828707176794 +/- 0.01050\n",
      "\tBest parameters for pa_unigrams_bigrams_trigrams_tfidf: {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_no_vectorization</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_unigrams_title</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_unigrams_title</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>{'multinomialnb__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svc_unigrams_title</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pa_unigrams_title</td>\n",
       "      <td>0.775444</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit_unigrams</td>\n",
       "      <td>0.908477</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nb_unigrams</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc_unigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa_unigrams</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit_unigrams_tfidf</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nb_unigrams_tfidf</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>svc_unigrams_tfidf</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pa_unigrams_tfidf</td>\n",
       "      <td>0.892723</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit_unigrams_tfidf_all_features</td>\n",
       "      <td>0.909477</td>\n",
       "      <td>{'penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logit_bigrams</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>{'logisticregression__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb_bigrams</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>{'multinomialnb__alpha': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svc_bigrams</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pa_bigrams</td>\n",
       "      <td>0.903476</td>\n",
       "      <td>{'passiveaggressiveclassifier__C': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svc_bigrams_tfidf</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pa_bigrams_tfidf</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logit_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svc_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.936484</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pa_unigrams_bigrams_tfidf</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pa_trigrams_tfidf</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pa_bigrams_trigrams_tfidf</td>\n",
       "      <td>0.923481</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pa_unigrams_bigrams_trigrams_tfidf</td>\n",
       "      <td>0.931483</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model_name  best_acc  \\\n",
       "0               logit_no_vectorization  0.574394   \n",
       "1                 logit_unigrams_title  0.799200   \n",
       "2                    nb_unigrams_title  0.806702   \n",
       "3                   svc_unigrams_title  0.771693   \n",
       "4                    pa_unigrams_title  0.775444   \n",
       "5                       logit_unigrams  0.908477   \n",
       "6                          nb_unigrams  0.887722   \n",
       "7                         svc_unigrams  0.892473   \n",
       "8                          pa_unigrams  0.880970   \n",
       "9                 logit_unigrams_tfidf  0.909977   \n",
       "10                   nb_unigrams_tfidf  0.855964   \n",
       "11                  svc_unigrams_tfidf  0.928732   \n",
       "12                   pa_unigrams_tfidf  0.892723   \n",
       "13   logit_unigrams_tfidf_all_features  0.909477   \n",
       "14                       logit_bigrams  0.910228   \n",
       "15                          nb_bigrams  0.892473   \n",
       "16                         svc_bigrams  0.893973   \n",
       "17                          pa_bigrams  0.903476   \n",
       "18                   svc_bigrams_tfidf  0.929482   \n",
       "19                    pa_bigrams_tfidf  0.929732   \n",
       "20        logit_unigrams_bigrams_tfidf  0.898975   \n",
       "21          svc_unigrams_bigrams_tfidf  0.936484   \n",
       "22           pa_unigrams_bigrams_tfidf  0.938985   \n",
       "23                   pa_trigrams_tfidf  0.901975   \n",
       "24           pa_bigrams_trigrams_tfidf  0.923481   \n",
       "25  pa_unigrams_bigrams_trigrams_tfidf  0.931483   \n",
       "\n",
       "                              best_params  \n",
       "0                       {'penalty': 'l2'}  \n",
       "1   {'logisticregression__penalty': 'l2'}  \n",
       "2             {'multinomialnb__alpha': 1}  \n",
       "3                                      {}  \n",
       "4                                      {}  \n",
       "5   {'logisticregression__penalty': 'l2'}  \n",
       "6           {'multinomialnb__alpha': 0.5}  \n",
       "7                                      {}  \n",
       "8   {'passiveaggressiveclassifier__C': 1}  \n",
       "9                                      {}  \n",
       "10          {'multinomialnb__alpha': 0.5}  \n",
       "11                                     {}  \n",
       "12  {'passiveaggressiveclassifier__C': 1}  \n",
       "13                      {'penalty': 'l2'}  \n",
       "14  {'logisticregression__penalty': 'l2'}  \n",
       "15          {'multinomialnb__alpha': 0.5}  \n",
       "16                                     {}  \n",
       "17  {'passiveaggressiveclassifier__C': 2}  \n",
       "18                                     {}  \n",
       "19                                     {}  \n",
       "20                                     {}  \n",
       "21                                     {}  \n",
       "22                                     {}  \n",
       "23                                     {}  \n",
       "24                                     {}  \n",
       "25                                     {}  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_unigrams_bigrams_trigrams_tfidf = {\n",
    "    \"model\": PassiveAggressiveClassifier(C=2), # seems to do the best,trying to save tim\n",
    "    \"name\": \"pa_unigrams_bigrams_trigrams_tfidf\",\n",
    "    \"params\": {},\n",
    "    \"vectorizer\": TfidfVectorizer(ngram_range=(1,3))\n",
    "}\n",
    "\n",
    "print(\"Running grid search and saving result....\")\n",
    "save_result(ngram_grid_search(train_all_features, pa_unigrams_bigrams_trigrams_tfidf))\n",
    "\n",
    "pd.read_csv(\"models/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth...=None, shuffle=True, tol=None,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"models/model_results.csv\")\n",
    "best_model = results[results[\"best_acc\"] == max(results[\"best_acc\"])][\"model_name\"].iloc[0]\n",
    "\n",
    "final_model = load(\"models/{}.joblib\".format(best_model))\n",
    "\n",
    "final_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REAL    1162\n",
       "FAKE    1159\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_features = pd.read_csv(\"data/test_all_features.csv\")\n",
    "preds = pd.Series(final_model.predict(test_all_features[\"clean_combined\"])).rename(\"prediction\")\n",
    "\n",
    "preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8430</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1220</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9624</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8211</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4099</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>878</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5304</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>92</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1545</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3517</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10414</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6109</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6456</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7431</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9078</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8054</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5252</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>219</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10223</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6802</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6951</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3003</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6855</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3548</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2050</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>6457</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>7030</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>9013</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>9509</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>3825</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>4515</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>2747</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>6516</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>9636</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>7398</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>3717</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>5205</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>6696</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>7991</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>1303</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>9051</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>10200</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>10009</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>4214</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>2316</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>8411</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>6143</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>3262</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>9337</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>8737</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>4490</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>8062</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>8622</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>4021</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>4330</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2321 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID prediction\n",
       "0     10498       FAKE\n",
       "1      2439       REAL\n",
       "2       864       REAL\n",
       "3      4128       REAL\n",
       "4       662       REAL\n",
       "5      8430       FAKE\n",
       "6      1220       REAL\n",
       "7      9624       FAKE\n",
       "8      8211       FAKE\n",
       "9      4099       REAL\n",
       "10      878       REAL\n",
       "11     5304       FAKE\n",
       "12       92       REAL\n",
       "13     1545       REAL\n",
       "14     3517       REAL\n",
       "15    10414       FAKE\n",
       "16     6109       FAKE\n",
       "17     6456       FAKE\n",
       "18     7431       REAL\n",
       "19     9078       FAKE\n",
       "20     8054       REAL\n",
       "21     5252       REAL\n",
       "22      219       REAL\n",
       "23    10223       FAKE\n",
       "24     6802       FAKE\n",
       "25     6951       REAL\n",
       "26     3003       REAL\n",
       "27     6855       FAKE\n",
       "28     3548       REAL\n",
       "29     2050       REAL\n",
       "...     ...        ...\n",
       "2291   6457       REAL\n",
       "2292   7030       FAKE\n",
       "2293   9013       FAKE\n",
       "2294   9509       FAKE\n",
       "2295   3825       REAL\n",
       "2296   4515       REAL\n",
       "2297   2747       REAL\n",
       "2298   6516       FAKE\n",
       "2299   9636       REAL\n",
       "2300   7398       FAKE\n",
       "2301   3717       REAL\n",
       "2302   5205       REAL\n",
       "2303   6696       FAKE\n",
       "2304   7991       FAKE\n",
       "2305   1303       REAL\n",
       "2306   9051       FAKE\n",
       "2307  10200       FAKE\n",
       "2308  10009       FAKE\n",
       "2309   4214       REAL\n",
       "2310   2316       REAL\n",
       "2311   8411       FAKE\n",
       "2312   6143       FAKE\n",
       "2313   3262       REAL\n",
       "2314   9337       FAKE\n",
       "2315   8737       FAKE\n",
       "2316   4490       REAL\n",
       "2317   8062       FAKE\n",
       "2318   8622       FAKE\n",
       "2319   4021       REAL\n",
       "2320   4330       REAL\n",
       "\n",
       "[2321 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([test_all_features[\"ID\"], preds], axis=1).to_csv(\"data/predictions.csv\",index=False)\n",
    "pd.read_csv(\"data/predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_copy = load(\"models/{}.joblib\".format(best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred   obs\n",
       "0   REAL  REAL\n",
       "1   REAL  REAL\n",
       "2   REAL  REAL\n",
       "3   FAKE  FAKE\n",
       "4   REAL  REAL\n",
       "5   REAL  FAKE\n",
       "6   FAKE  FAKE\n",
       "7   REAL  REAL\n",
       "8   FAKE  FAKE\n",
       "9   REAL  REAL\n",
       "10  REAL  REAL\n",
       "11  FAKE  FAKE\n",
       "12  REAL  FAKE\n",
       "13  FAKE  FAKE\n",
       "14  REAL  REAL\n",
       "15  REAL  FAKE\n",
       "16  REAL  FAKE\n",
       "17  REAL  REAL\n",
       "18  REAL  REAL\n",
       "19  REAL  FAKE"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(data, model):\n",
    "    X = data[\"clean_combined\"]\n",
    "    y = data[\"label\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=20193105)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = pd.Series(model.predict(X_test)).rename(\"pred\")\n",
    "    obs = y_test.reset_index()[\"label\"].rename(\"obs\")\n",
    "    \n",
    "    return pd.concat([preds, obs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-4bfffc5923e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mholdout_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_all_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_model_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mTP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"FAKE\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"obs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"FAKE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mFP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"FAKE\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"obs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"REAL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"REAL\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mholdout_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"obs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"REAL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1574\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m   1575\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "holdout_results = evaluate_model(train_all_features, final_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 29, 393, 25)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_tp = lambda row: row[\"pred\"] == \"FAKE\" and row[\"obs\"] == \"FAKE\"\n",
    "is_fp = lambda row: row[\"pred\"] == \"FAKE\" and row[\"obs\"] == \"REAL\"\n",
    "is_tn = lambda row: row[\"pred\"] == \"REAL\" and row[\"obs\"] == \"REAL\"\n",
    "is_fn = lambda row: row[\"pred\"] == \"REAL\" and row[\"obs\"] == \"FAKE\"\n",
    "\n",
    "TP = sum(holdout_results.apply(is_tp, axis=1))\n",
    "FP = sum(holdout_results.apply(is_fp, axis=1))\n",
    "TN = sum(holdout_results.apply(is_tn, axis=1))\n",
    "FN = sum(holdout_results.apply(is_fn, axis=1))\n",
    "\n",
    "(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25bc1d7bfd0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = [[TP, FP], \n",
    "         [FN, TN]]\n",
    "df_cm = pd.DataFrame(array,\n",
    "                     index = [i for i in [\"FAKE\", \"REAL\"]],\n",
    "                     columns = [i for i in [\"FAKE\", \"REAL\"]])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
